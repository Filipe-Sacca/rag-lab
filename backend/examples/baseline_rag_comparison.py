"""
Compara√ß√£o: C√≥digo Antigo vs Refatorado com PromptTemplate

Este arquivo mostra a diferen√ßa entre usar f-strings simples
e usar PromptTemplate do LangChain.
"""

import asyncio
import time
from typing import Dict, Any

from core.llm import get_llm
from core.embeddings import get_query_embedding_model
from core.vector_store import get_vector_store
from core.prompts import BASELINE_RAG_TEMPLATE  # ‚Üê Novo import
from config import settings


# ============================================
# VERS√ÉO ANTIGA (F-STRING)
# ============================================


def _build_prompt_old(query: str, context: str) -> str:
    """
    Vers√£o antiga: Prompt hard-coded como f-string.

    Problemas:
    - ‚ùå Hard-coded no c√≥digo
    - ‚ùå Dif√≠cil de reutilizar
    - ‚ùå N√£o valida vari√°veis
    - ‚ùå Dif√≠cil de testar isoladamente
    """
    return f"""Voce e um assistente especializado em responder perguntas baseado APENAS no contexto fornecido.

CONTEXTO:
{context}

PERGUNTA: {query}

INSTRUCOES:
1. Responda a pergunta usando APENAS as informacoes do contexto acima
2. Se a resposta nao estiver no contexto, diga: "Nao encontrei informacoes suficientes no contexto para responder essa pergunta."
3. Seja preciso e objetivo
4. Cite trechos relevantes quando apropriado

RESPOSTA:"""


async def baseline_rag_old(query: str, top_k: int = 5) -> Dict[str, Any]:
    """
    Baseline RAG - Vers√£o Antiga com F-String

    Usa fun√ß√£o _build_prompt_old() com f-string simples.
    """
    start_time = time.time()

    # Initialize
    vector_store = get_vector_store()
    llm = get_llm()

    # Retrieve
    retrieved_docs = vector_store.similarity_search_with_score(query, k=top_k)

    # Build context
    context = "\n\n".join([doc.page_content for doc, _ in retrieved_docs])

    # Build prompt (vers√£o antiga) ‚Üê AQUI
    prompt = _build_prompt_old(query, context)

    # Generate
    response = llm.invoke(prompt)

    return {
        "query": query,
        "answer": response.content,
        "sources": [
            {"content": doc.page_content, "score": float(score)}
            for doc, score in retrieved_docs
        ],
        "latency_ms": round((time.time() - start_time) * 1000, 2),
        "method": "old_fstring",
    }


# ============================================
# VERS√ÉO NOVA (PROMPTTEMPLATE)
# ============================================


async def baseline_rag_new(query: str, top_k: int = 5) -> Dict[str, Any]:
    """
    Baseline RAG - Vers√£o Nova com PromptTemplate

    Usa BASELINE_RAG_TEMPLATE do core.prompts.

    Vantagens:
    - ‚úÖ Prompt centralizado e reutiliz√°vel
    - ‚úÖ Valida√ß√£o autom√°tica de vari√°veis
    - ‚úÖ F√°cil de testar isoladamente
    - ‚úÖ F√°cil de versionar e modificar
    """
    start_time = time.time()

    # Initialize
    vector_store = get_vector_store()
    llm = get_llm()

    # Retrieve
    retrieved_docs = vector_store.similarity_search_with_score(query, k=top_k)

    # Build context
    context = "\n\n".join([doc.page_content for doc, _ in retrieved_docs])

    # Build prompt (vers√£o nova) ‚Üê MUDAN√áA AQUI
    prompt = BASELINE_RAG_TEMPLATE.format(context=context, query=query)

    # Generate
    response = llm.invoke(prompt)

    return {
        "query": query,
        "answer": response.content,
        "sources": [
            {"content": doc.page_content, "score": float(score)}
            for doc, score in retrieved_docs
        ],
        "latency_ms": round((time.time() - start_time) * 1000, 2),
        "method": "new_prompttemplate",
    }


# ============================================
# VERS√ÉO COM CHAIN PATTERN (MAIS MODERNA)
# ============================================


async def baseline_rag_chain(query: str, top_k: int = 5) -> Dict[str, Any]:
    """
    Baseline RAG - Vers√£o com LCEL (LangChain Expression Language)

    Usa chain pattern para composi√ß√£o de opera√ß√µes.
    Mais moderno e idiom√°tico no LangChain.

    Vantagens:
    - ‚úÖ Todas as vantagens do PromptTemplate
    - ‚úÖ Composi√ß√£o declarativa de opera√ß√µes
    - ‚úÖ Mais f√°cil adicionar logging/tracing
    - ‚úÖ Suporte a streaming autom√°tico
    """
    start_time = time.time()

    # Initialize
    vector_store = get_vector_store()
    llm = get_llm()

    # Build chain: Template ‚Üí LLM
    chain = BASELINE_RAG_TEMPLATE | llm

    # Retrieve
    retrieved_docs = vector_store.similarity_search_with_score(query, k=top_k)

    # Build context
    context = "\n\n".join([doc.page_content for doc, _ in retrieved_docs])

    # Execute chain ‚Üê DIFEREN√áA AQUI
    response = chain.invoke({"context": context, "query": query})

    return {
        "query": query,
        "answer": response.content,
        "sources": [
            {"content": doc.page_content, "score": float(score)}
            for doc, score in retrieved_docs
        ],
        "latency_ms": round((time.time() - start_time) * 1000, 2),
        "method": "chain_pattern",
    }


# ============================================
# COMPARA√á√ÉO E TESTES
# ============================================


async def compare_all_methods(query: str, top_k: int = 5):
    """
    Compara todas as 3 vers√µes lado a lado.

    Args:
        query: Pergunta para testar
        top_k: N√∫mero de documentos a recuperar

    Returns:
        Resultados das 3 vers√µes para compara√ß√£o
    """
    print("=" * 80)
    print("COMPARA√á√ÉO: Antigas vs Nova Implementa√ß√£o")
    print("=" * 80)

    # M√©todo 1: F-String (Antigo)
    print("\n1Ô∏è‚É£ M√âTODO ANTIGO (F-STRING)")
    print("-" * 80)
    result_old = await baseline_rag_old(query, top_k)
    print(f"Resposta: {result_old['answer'][:200]}...")
    print(f"Lat√™ncia: {result_old['latency_ms']}ms")
    print(f"Fontes: {len(result_old['sources'])} documentos")

    # M√©todo 2: PromptTemplate (Novo)
    print("\n2Ô∏è‚É£ M√âTODO NOVO (PROMPTTEMPLATE)")
    print("-" * 80)
    result_new = await baseline_rag_new(query, top_k)
    print(f"Resposta: {result_new['answer'][:200]}...")
    print(f"Lat√™ncia: {result_new['latency_ms']}ms")
    print(f"Fontes: {len(result_new['sources'])} documentos")

    # M√©todo 3: Chain Pattern (Mais Moderno)
    print("\n3Ô∏è‚É£ M√âTODO CHAIN PATTERN (LCEL)")
    print("-" * 80)
    result_chain = await baseline_rag_chain(query, top_k)
    print(f"Resposta: {result_chain['answer'][:200]}...")
    print(f"Lat√™ncia: {result_chain['latency_ms']}ms")
    print(f"Fontes: {len(result_chain['sources'])} documentos")

    # Compara√ß√£o
    print("\n" + "=" * 80)
    print("AN√ÅLISE COMPARATIVA")
    print("=" * 80)

    # Verificar se respostas s√£o id√™nticas
    answers_identical = (
        result_old["answer"] == result_new["answer"] == result_chain["answer"]
    )

    print(f"\n‚úÖ Respostas id√™nticas: {answers_identical}")

    if not answers_identical:
        print("‚ö†Ô∏è ATEN√á√ÉO: As respostas diferem!")
        print(
            "   Isso pode acontecer devido a n√£o-determinismo do LLM (temperature > 0)"
        )
        print("   Para compara√ß√£o exata, use temperature=0.0")

    # Comparar performance
    print("\nüìä PERFORMANCE:")
    print(f"   F-String:        {result_old['latency_ms']}ms")
    print(f"   PromptTemplate:  {result_new['latency_ms']}ms")
    print(f"   Chain Pattern:   {result_chain['latency_ms']}ms")

    overhead = result_new["latency_ms"] - result_old["latency_ms"]
    print(f"\n   Overhead PromptTemplate: {overhead:.2f}ms")
    print(
        f"   (Overhead √© neglig√≠vel: {overhead/result_old['latency_ms']*100:.2f}% do tempo total)"
    )

    return {
        "old_fstring": result_old,
        "new_prompttemplate": result_new,
        "chain_pattern": result_chain,
        "answers_identical": answers_identical,
    }


# ============================================
# EXEMPLO DE USO
# ============================================


async def main():
    """Exemplo de compara√ß√£o das 3 vers√µes"""

    query = "O que √© Python?"

    print("\nüîç QUERY:", query)

    results = await compare_all_methods(query, top_k=3)

    # Resumo
    print("\n" + "=" * 80)
    print("RESUMO")
    print("=" * 80)

    print("\nüìù RECOMENDA√á√ÉO:")
    print("   ‚úÖ Use PromptTemplate (m√©todo 2) ou Chain Pattern (m√©todo 3)")
    print("   ‚úÖ Evite F-Strings para prompts (m√©todo 1)")
    print("\nüí° VANTAGENS DO PROMPTTEMPLATE:")
    print("   ‚Ä¢ Valida√ß√£o autom√°tica de vari√°veis")
    print("   ‚Ä¢ Reutiliza√ß√£o em m√∫ltiplos lugares")
    print("   ‚Ä¢ F√°cil versionamento e testes")
    print("   ‚Ä¢ Overhead de performance neglig√≠vel")
    print("   ‚Ä¢ Melhor organiza√ß√£o do c√≥digo")

    print("\nüöÄ PR√ìXIMOS PASSOS:")
    print("   1. Adicionar BASELINE_RAG_TEMPLATE em core/prompts.py")
    print("   2. Refatorar baseline_rag.py para usar o template")
    print("   3. Implementar HyDE, Reranking e Agentic com templates")
    print("   4. Adicionar testes para prompts")


if __name__ == "__main__":
    # Para executar:
    # python -m examples.baseline_rag_comparison
    asyncio.run(main())
