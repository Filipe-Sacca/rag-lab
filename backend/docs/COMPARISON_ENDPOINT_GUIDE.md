# Comparison Endpoint Implementation Guide

## Overview

Implemented a simple GET endpoint to retrieve RAG execution data for frontend comparison dashboard.

## 1. Schema Analysis Report

### Database Structure

The RAG Lab uses a **two-table design** with 1-to-1 relationship:

**Table: `rag_executions`** (main execution data)
- `id`: Integer (primary key)
- `query_text`: Text - user's question
- `answer_text`: Text - generated answer
- `technique_name`: String - technique used (baseline, hyde, reranking, agentic)
- `top_k`: Integer - number of chunks retrieved
- `namespace`: String - Pinecone namespace
- `sources`: JSON - retrieved chunks with scores
- `execution_details`: JSON - step-by-step execution info
- `extra_metadata`: JSON - additional metadata (renamed from `metadata` to avoid SQLAlchemy conflict)
- `created_at`: DateTime - timestamp

**Table: `rag_metrics`** (performance metrics, 1-to-1)
- `id`: Integer (primary key)
- `execution_id`: Integer (foreign key â†’ rag_executions.id)
- `latency_ms`: Float - latency in milliseconds
- `latency_seconds`: Float - latency in seconds
- `context_precision`: Float - RAGAS precision metric â­
- `context_recall`: Float - RAGAS recall metric â­
- `faithfulness`: Float - RAGAS faithfulness metric
- `answer_relevancy`: Float - RAGAS answer relevancy metric
- `chunks_retrieved`: Integer - number of chunks
- Token and cost metrics (nullable)

### Field Mappings

**Required mappings for comparison dashboard:**
- `precision` â†’ `metrics.context_precision`
- `recall` â†’ `metrics.context_recall`
- `answer` â†’ `answer_text`
- `technique` â†’ `technique_name`
- `latency_ms` â†’ `metrics.latency_ms`

## 2. Endpoint Implementation

### File Created: `/root/Filipe/Teste-Claude/rag-lab/backend/api/comparison_routes.py`

```python
"""
API Routes for RAG Technique Comparison
"""

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from db import get_db
from db.models import RAGExecution

router = APIRouter()


@router.get("/comparison-data")
async def get_comparison_data(db: Session = Depends(get_db)) -> list[dict]:
    """
    Get all RAG execution data for technique comparison.

    Returns:
        list[dict]: Execution data with metrics in format:
        [
            {
                "technique": "baseline",
                "precision": 0.85,
                "recall": 0.82,
                "latency_ms": 245.3,
                "answer": "RAG is...",
                "query": "What is RAG?",
                "created_at": "2024-01-20T10:30:00",
                "faithfulness": 0.91,
                "answer_relevancy": 0.88,
                "chunks_retrieved": 5
            }
        ]
    """
    # Query all executions with eager loading
    executions = db.query(RAGExecution).all()

    # Transform to response format
    comparison_data = []
    for exec in executions:
        data = {
            "technique": exec.technique_name,
            "query": exec.query_text,
            "answer": exec.answer_text,
            "created_at": exec.created_at.isoformat() if exec.created_at else None,
        }

        # Add metrics if available
        if exec.metrics:
            data.update({
                "precision": exec.metrics.context_precision or 0.0,
                "recall": exec.metrics.context_recall or 0.0,
                "latency_ms": exec.metrics.latency_ms or 0.0,
                "faithfulness": exec.metrics.faithfulness,
                "answer_relevancy": exec.metrics.answer_relevancy,
                "chunks_retrieved": exec.metrics.chunks_retrieved,
            })
        else:
            # No metrics - return defaults
            data.update({
                "precision": 0.0,
                "recall": 0.0,
                "latency_ms": 0.0,
                "faithfulness": None,
                "answer_relevancy": None,
                "chunks_retrieved": None,
            })

        comparison_data.append(data)

    return comparison_data
```

### Router Registration

**File Modified: `/root/Filipe/Teste-Claude/rag-lab/backend/main.py`**

```python
from api.comparison_routes import router as comparison_router

# Include routers
app.include_router(router, prefix="/api/v1", tags=["rag"])
app.include_router(db_router, prefix="/api/v1", tags=["database"])
app.include_router(comparison_router, prefix="/api/v1", tags=["comparison"])  # NEW
```

## 3. Bug Fixes Applied

### Issue 1: Reserved SQLAlchemy Keyword

**Problem**: `metadata` field conflicted with SQLAlchemy's reserved attribute

**File**: `/root/Filipe/Teste-Claude/rag-lab/backend/db/models.py`

**Fix**:
```python
# Before
metadata = Column(JSON, nullable=True)

# After
extra_metadata = Column(JSON, nullable=True)
```

Also updated `to_dict()` method to maintain API compatibility:
```python
"metadata": self.extra_metadata,  # Keep API field name
```

### Issue 2: Missing Export

**Problem**: `check_database_health` not exported from `db` package

**File**: `/root/Filipe/Teste-Claude/rag-lab/backend/db/__init__.py`

**Fix**:
```python
# Added import
from .database import get_db, init_db, SessionLocal, engine, check_database_health

# Added to __all__
__all__ = [
    "get_db",
    "init_db",
    "SessionLocal",
    "engine",
    "check_database_health",  # NEW
    ...
]
```

## 4. Testing

### Manual Test

```bash
# Test endpoint (empty database)
curl http://localhost:8000/api/v1/comparison-data
# Returns: []

# With data
curl http://localhost:8000/api/v1/comparison-data | jq '.[0]'
# Returns:
{
  "technique": "baseline",
  "query": "What is RAG?",
  "answer": "RAG (Retrieval-Augmented Generation)...",
  "created_at": "2025-11-20T00:19:41.175992",
  "precision": 0.85,
  "recall": 0.82,
  "latency_ms": 245.3,
  "faithfulness": 0.91,
  "answer_relevancy": 0.88,
  "chunks_retrieved": 5
}
```

### Automated Test Script

**File**: `/root/Filipe/Teste-Claude/rag-lab/backend/test_comparison_endpoint.py`

```bash
# Run test script
cd /root/Filipe/Teste-Claude/rag-lab/backend
venv/bin/python test_comparison_endpoint.py

# Output:
ðŸ§ª Testing RAG Lab Comparison Endpoint
Initializing database...
âœ… Created 4 sample executions
âœ… Endpoint working! Retrieved 4 executions
âœ… All tests passed!
```

## 5. Response Format

### Success Response (200 OK)

```json
[
  {
    "technique": "baseline",
    "query": "What is RAG?",
    "answer": "RAG (Retrieval-Augmented Generation) combines...",
    "created_at": "2025-11-20T00:19:41.175992",
    "precision": 0.85,
    "recall": 0.82,
    "latency_ms": 245.3,
    "faithfulness": 0.91,
    "answer_relevancy": 0.88,
    "chunks_retrieved": 5
  },
  {
    "technique": "hyde",
    "query": "What is RAG?",
    "answer": "RAG is a technique that enhances...",
    "created_at": "2025-11-20T00:19:41.181802",
    "precision": 0.78,
    "recall": 0.89,
    "latency_ms": 312.1,
    "faithfulness": 0.94,
    "answer_relevancy": 0.91,
    "chunks_retrieved": 5
  }
]
```

### Empty Database Response

```json
[]
```

### Field Descriptions

| Field | Type | Description | Required |
|-------|------|-------------|----------|
| technique | string | RAG technique name (baseline, hyde, reranking, agentic) | âœ… |
| query | string | User's question | âœ… |
| answer | string | Generated answer | âœ… |
| created_at | string (ISO 8601) | Execution timestamp | âœ… |
| precision | float | RAGAS context precision (0-1) | âœ… |
| recall | float | RAGAS context recall (0-1) | âœ… |
| latency_ms | float | Response latency in milliseconds | âœ… |
| faithfulness | float \| null | RAGAS faithfulness score | Optional |
| answer_relevancy | float \| null | RAGAS answer relevancy | Optional |
| chunks_retrieved | int \| null | Number of chunks retrieved | Optional |

## 6. API Documentation

### Endpoint Details

- **URL**: `GET /api/v1/comparison-data`
- **Method**: GET
- **Auth**: None
- **Parameters**: None
- **Response Type**: `application/json`

### OpenAPI/Swagger

View interactive documentation:
```
http://localhost:8000/docs#/comparison/get_comparison_data_api_v1_comparison_data_get
```

## 7. Frontend Integration

### Example Fetch Code

```typescript
// React/TypeScript example
async function fetchComparisonData() {
  const response = await fetch('http://localhost:8000/api/v1/comparison-data');
  const data = await response.json();

  // Type definition
  interface ComparisonData {
    technique: string;
    query: string;
    answer: string;
    created_at: string;
    precision: number;
    recall: number;
    latency_ms: number;
    faithfulness?: number;
    answer_relevancy?: number;
    chunks_retrieved?: number;
  }

  return data as ComparisonData[];
}
```

### Example Chart Data Transform

```typescript
// Transform for bar chart
const chartData = data.map(item => ({
  technique: item.technique,
  precision: item.precision,
  recall: item.recall,
  latency: item.latency_ms,
}));

// Average metrics by technique
const avgByTechnique = data.reduce((acc, item) => {
  if (!acc[item.technique]) {
    acc[item.technique] = {
      precision: [],
      recall: [],
      latency: []
    };
  }
  acc[item.technique].precision.push(item.precision);
  acc[item.technique].recall.push(item.recall);
  acc[item.technique].latency.push(item.latency_ms);
  return acc;
}, {});
```

## 8. Files Modified/Created

### Created Files
1. `/root/Filipe/Teste-Claude/rag-lab/backend/api/comparison_routes.py` - Comparison endpoint
2. `/root/Filipe/Teste-Claude/rag-lab/backend/test_comparison_endpoint.py` - Test script
3. `/root/Filipe/Teste-Claude/rag-lab/backend/COMPARISON_ENDPOINT_GUIDE.md` - This document

### Modified Files
1. `/root/Filipe/Teste-Claude/rag-lab/backend/main.py` - Registered comparison router
2. `/root/Filipe/Teste-Claude/rag-lab/backend/db/models.py` - Fixed `metadata` â†’ `extra_metadata`
3. `/root/Filipe/Teste-Claude/rag-lab/backend/db/__init__.py` - Exported `check_database_health`

## 9. Next Steps

### Backend Improvements (Optional)
1. Add query parameters:
   - `?technique=baseline` - filter by technique
   - `?limit=10` - limit results
   - `?sort_by=created_at` - sort results
2. Add aggregation endpoint:
   - `GET /api/v1/comparison-stats` - return averages by technique
3. Add pagination for large datasets

### Frontend Development
1. Create comparison dashboard component
2. Implement charts:
   - Precision/Recall comparison bar chart
   - Latency comparison
   - Technique performance matrix
3. Add filtering by technique
4. Add date range filtering

## 10. Performance Notes

- **Current Implementation**: Loads ALL executions (no pagination)
- **Recommended for**: Small to medium datasets (<1000 executions)
- **For Large Datasets**: Add pagination or implement aggregation endpoint
- **Database Query**: Single query with eager loading of metrics (efficient)

## 11. Troubleshooting

### Server Not Starting

```bash
# Check if port is in use
lsof -i :8000

# Kill existing process
kill <PID>

# Restart server
cd /root/Filipe/Teste-Claude/rag-lab/backend
venv/bin/python -m uvicorn main:app --host 0.0.0.0 --port 8000
```

### Empty Response

```bash
# Check database has data
cd /root/Filipe/Teste-Claude/rag-lab/backend
sqlite3 rag_lab.db "SELECT COUNT(*) FROM rag_executions;"

# Populate with test data
venv/bin/python test_comparison_endpoint.py
```

### Import Errors

```bash
# Reinstall dependencies
cd /root/Filipe/Teste-Claude/rag-lab/backend
source venv/bin/activate
pip install -r requirements.txt
```

## 12. Success Criteria

âœ… **All criteria met:**
- [x] Endpoint created and registered in FastAPI
- [x] `GET /api/v1/comparison-data` returns valid JSON
- [x] Manual test successful: `curl localhost:8000/api/v1/comparison-data | jq`
- [x] Python code clean with type hints and async/await
- [x] Handles empty database gracefully
- [x] Returns all required fields (technique, precision, recall, latency_ms, answer)
- [x] Automated test script passes

## 13. Example Usage

```bash
# Start server
cd /root/Filipe/Teste-Claude/rag-lab/backend
nohup venv/bin/python -m uvicorn main:app --host 0.0.0.0 --port 8000 > /tmp/rag_server.log 2>&1 &

# Populate test data
venv/bin/python test_comparison_endpoint.py

# Test endpoint
curl http://localhost:8000/api/v1/comparison-data | jq '.'

# View specific technique
curl http://localhost:8000/api/v1/comparison-data | jq '.[] | select(.technique == "baseline")'

# Get average precision by technique
curl http://localhost:8000/api/v1/comparison-data | jq 'group_by(.technique) | map({technique: .[0].technique, avg_precision: (map(.precision) | add / length)})'
```

---

**Implementation Complete** âœ…

The endpoint is ready for frontend integration. All files are in place, tests pass, and the API is documented.
