# HyDE - Hypothetical Document Embeddings

## ğŸ“‹ DefiniÃ§Ã£o

**HyDE** (Hypothetical Document Embeddings) Ã© uma tÃ©cnica avanÃ§ada de otimizaÃ§Ã£o de query que **inverte a lÃ³gica tradicional do RAG**.

Ao invÃ©s de buscar com a pergunta, o LLM **gera uma resposta hipotÃ©tica** (mesmo que incorreta), e essa resposta Ã© usada para buscar documentos similares.

**Insight**: Respostas sÃ£o semanticamente mais prÃ³ximas de documentos do que perguntas.

---

## ğŸ”„ Como Funciona

### Pipeline Completo

```
1. GERAÃ‡ÃƒO HIPOTÃ‰TICA (Novo!)
   â”œâ”€ User pergunta: "Qual o lucro Q3?"
   â”œâ”€ LLM gera resposta hipotÃ©tica SEM ver documentos
   â””â”€ HipÃ³tese: "O lucro do Q3 foi de aproximadamente R$ 3 bilhÃµes..."

2. EMBEDDING DA HIPÃ“TESE (Diferente!)
   â”œâ”€ Gerar embedding da RESPOSTA (nÃ£o da pergunta)
   â””â”€ Vector: [0.145, -0.891, 0.234, ...]

3. RECUPERAÃ‡ÃƒO
   â”œâ”€ Busca de similaridade no Pinecone
   â””â”€ Retornar top-k chunks

4. GERAÃ‡ÃƒO FINAL
   â”œâ”€ Montar prompt: [Chunks] + [Query Original]
   â”œâ”€ LLM gera resposta REAL (baseada nos chunks)
   â””â”€ Retornar resposta final
```

### ComparaÃ§Ã£o Visual

**Baseline RAG:**
```
Query: "Qual o lucro?"
  â†“ [Embed]
Vector da PERGUNTA â†’ Busca â†’ Chunks
```

**HyDE:**
```
Query: "Qual o lucro?"
  â†“ [LLM gera]
HipÃ³tese: "O lucro foi R$ 3bi com crescimento..."
  â†“ [Embed]
Vector da RESPOSTA â†’ Busca â†’ Chunks (melhores!)
```

---

## ğŸ’¡ Por Que Funciona?

### Problema do Baseline

```python
# Embedding da pergunta
query = "Qual foi o lucro?"
query_vector = embed("Qual foi o lucro?")

# Documentos dizem:
doc = "A receita lÃ­quida do trimestre atingiu R$ 3 bilhÃµes..."

# âŒ DistÃ¢ncia semÃ¢ntica:
# "Qual foi o lucro?" â‰ â‰  "receita lÃ­quida atingiu R$ 3bi"
# (Pergunta vs DeclaraÃ§Ã£o = estilos diferentes)
```

### SoluÃ§Ã£o HyDE

```python
# LLM gera hipÃ³tese (estilo declarativo)
hypothesis = "O lucro do trimestre foi de aproximadamente R$ 3 bilhÃµes"

# Embed a hipÃ³tese
hyp_vector = embed(hypothesis)

# âœ… Agora:
# "lucro foi R$ 3 bilhÃµes" â‰ˆâ‰ˆ "receita lÃ­quida atingiu R$ 3 bilhÃµes"
# (DeclaraÃ§Ã£o vs DeclaraÃ§Ã£o = semanticamente prÃ³ximas!)
```

**Resultado**: Similarity score sobe de 0.75 â†’ 0.92

---

## ğŸ”¬ Exemplo PrÃ¡tico Detalhado

### Caso 1: Query AmbÃ­gua

**Input:**
```
Query: "Como melhorar performance?"
```

**Baseline RAG:**
```python
# Embedding direto da query
results = search(embed("Como melhorar performance?"))

# Chunks recuperados (confusos):
1. "Performance de vendas aumentou 20%..."      (marketing)
2. "OtimizaÃ§Ã£o de cÃ³digo reduz latÃªncia..."     (tech)
3. "Performance financeira do Q3..."            (finance)

# âŒ Sem contexto, pega de tudo
```

**HyDE:**
```python
# 1. LLM gera hipÃ³tese (assume contexto tech)
hypothesis = llm.generate("""
Pergunta: Como melhorar performance?
Gere uma resposta hipotÃ©tica tÃ©cnica:
""")
# â†’ "Para melhorar performance de sistemas, otimize queries SQL,
#    implemente cache Redis, use CDN para assets..."

# 2. Busca com a hipÃ³tese
results = search(embed(hypothesis))

# Chunks recuperados (focados):
1. "OtimizaÃ§Ã£o de queries SQL: indexaÃ§Ã£o..."    âœ…
2. "ImplementaÃ§Ã£o de cache Redis..."            âœ…
3. "CDN para distribuiÃ§Ã£o de assets..."         âœ…

# âœ… Contexto tÃ©cnico capturado!
```

**Melhoria**: Context Precision de 0.60 â†’ 0.85

---

### Caso 2: VocabulÃ¡rio Mismatch

**Input:**
```
Query: "Quanto a empresa faturou no Ãºltimo trimestre?"
```

**Baseline:**
```python
# Busca com "faturou"
results = search(embed("faturou no Ãºltimo trimestre"))

# Documentos usam "receita lÃ­quida", nÃ£o "faturou"
# âŒ Similarity baixa (0.68)
```

**HyDE:**
```python
# 1. HipÃ³tese usa vocabulÃ¡rio corporativo
hypothesis = "A receita lÃ­quida consolidada do terceiro
              trimestre atingiu R$ 3 bilhÃµes..."

# 2. Busca
results = search(embed(hypothesis))

# âœ… "receita lÃ­quida" match perfeito (0.94)
```

---

## âš™ï¸ ConfiguraÃ§Ã£o PadrÃ£o

| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Hypothesis Prompt** | "Gere resposta detalhada..." | Instruir LLM a ser especÃ­fico |
| **Hypothesis Length** | 100-200 tokens | Balance entre detalhe e custo |
| **LLM** | Gemini 2.5 Flash | RÃ¡pido e barato para hipÃ³teses |
| **Temperature** | 0.7 | Criatividade para preencher lacunas |
| **Top-K** | 5 chunks | Mesmo do baseline |
| **Fallback** | Baseline se hipÃ³tese vazia | SeguranÃ§a |

---

## âœ… Vantagens

### 1. Maior PrecisÃ£o SemÃ¢ntica
```
Melhoria tÃ­pica: +15-25% em similarity scores
Context Precision: 0.60 â†’ 0.85
```

### 2. Resolve VocabulÃ¡rio Mismatch
```
User: "faturamento"
Docs: "receita lÃ­quida"
â†’ HipÃ³tese usa termo correto automaticamente
```

### 3. Captura IntenÃ§Ã£o ImplÃ­cita
```
Query vaga: "Como estÃ¡ a empresa?"
HipÃ³tese especifica: "SituaÃ§Ã£o financeira, crescimento, desafios..."
â†’ Busca mais direcionada
```

### 4. Funciona com Queries Abertas
```
Baseline falha: "Explique machine learning"
HyDE gera: "Machine learning Ã©... utiliza algoritmos... exemplos..."
â†’ Busca captura material didÃ¡tico
```

### 5. Multi-Idioma Natural
```
Query PT: "Qual o lucro?"
HipÃ³tese PT: "O lucro foi..."
Docs EN: "Profit was..."
â†’ Embedding multilingual aproxima automaticamente
```

---

## âŒ Desvantagens

### 1. Custo 2x Maior
```
Baseline: 1 chamada LLM (geraÃ§Ã£o final)
HyDE: 2 chamadas LLM (hipÃ³tese + geraÃ§Ã£o final)

Custo: $0.002 â†’ $0.004 por query (+100%)
```

### 2. LatÃªncia Aumentada
```
Baseline: 1.2s
HyDE: 1.8-2.5s (+50-100%)

Breakdown:
- GeraÃ§Ã£o hipÃ³tese: +0.5-0.8s
- Resto: igual
```

### 3. Risco de ViÃ©s na HipÃ³tese
```
Query: "Qual a polÃ­tica de RH?"
HipÃ³tese (viÃ©s): "PolÃ­ticas progressivas de diversidade..."

âŒ Se docs nÃ£o falam de diversidade, busca falha
â†’ HipÃ³tese assumiu contexto errado
```

### 4. Queries Factuais Simples = Overhead DesnecessÃ¡rio
```
Query: "Qual o telefone?"
Baseline: Busca direto, retorna em 1s
HyDE: Gera hipÃ³tese desnecessÃ¡ria, +0.8s

âŒ Complexidade sem benefÃ­cio
```

### 5. DependÃªncia de Qualidade do Prompt
```python
# Prompt ruim:
"Responda a pergunta"
â†’ HipÃ³tese genÃ©rica, nÃ£o ajuda

# Prompt bom:
"Gere resposta tÃ©cnica detalhada com termos especÃ­ficos do domÃ­nio"
â†’ HipÃ³tese Ãºtil
```

### 6. NÃ£o Funciona para Lookup
```
Query: "Email do CEO"
HipÃ³tese: "O email do CEO Ã© ceo@empresa.com"

âŒ LLM inventou email (alucinaÃ§Ã£o)
â†’ Busca com dado falso
```

---

## ğŸ“Š MÃ©tricas Esperadas

### RAGAS Scores vs Baseline

| MÃ©trica | Baseline | HyDE | Î” |
|---------|----------|------|---|
| **Faithfulness** | 0.75-0.85 | 0.80-0.90 | +5-7% |
| **Answer Relevancy** | 0.70-0.85 | 0.85-0.95 | +15-20% |
| **Context Precision** | 0.60-0.75 | 0.75-0.90 | +20-25% |
| **Context Recall** | 0.50-0.70 | 0.55-0.75 | +5-10% |

### Performance

| MÃ©trica | Baseline | HyDE |
|---------|----------|------|
| **LatÃªncia** | 1.2-2.5s | 1.8-3.5s |
| **Custo/Query** | $0.001-0.003 | $0.003-0.006 |
| **Throughput** | ~40 q/min | ~25 q/min |

---

## ğŸ¯ Quando Usar HyDE

### âœ… Casos Ideais

**1. Queries Abertas e ExploratÃ³rias**
```
âœ… "Explique a estratÃ©gia de crescimento"
âœ… "Como funciona o processo de vendas?"
âœ… "Qual a visÃ£o da empresa sobre IA?"
```

**2. VocabulÃ¡rio TÃ©cnico EspecÃ­fico**
```
âœ… User: "quebrou" â†’ Doc: "falha crÃ­tica de sistema"
âœ… User: "gastou" â†’ Doc: "despesas operacionais"
```

**3. Queries AmbÃ­guas**
```
âœ… "Como melhorar performance?" (tech vs business)
âœ… "Status do projeto?" (qual projeto?)
```

**4. DomÃ­nios com JargÃ£o**
```
âœ… Medicina: sintomas â†’ diagnÃ³sticos
âœ… JurÃ­dico: questÃµes â†’ artigos de lei
âœ… Financeiro: perguntas â†’ relatÃ³rios tÃ©cnicos
```

**5. Conhecimento ImplÃ­cito**
```
âœ… "RegulamentaÃ§Ãµes aplicÃ¡veis"
   â†’ HipÃ³tese: "LGPD, SOX, ISO27001..." (contexto BR)
```

---

### âŒ Quando NÃƒO Usar

**1. Queries Factuais Simples**
```
âŒ "Qual o telefone?"
âŒ "EndereÃ§o da matriz"
âŒ "CÃ³digo do produto X"
â†’ Use: Baseline (mais rÃ¡pido)
```

**2. Lookup Direto**
```
âŒ "Email do CEO"
âŒ "PreÃ§o do plano premium"
â†’ Risco: LLM alucina dados especÃ­ficos
```

**3. Dados Temporais Precisos**
```
âŒ "PreÃ§o da aÃ§Ã£o hoje"
âŒ "Lucro exato Q3 2024"
â†’ HipÃ³tese pode usar data/valor errado
```

**4. Budget Limitado**
```
âŒ >10K queries/dia com budget apertado
â†’ Custo 2x pode ser proibitivo
```

**5. LatÃªncia CrÃ­tica**
```
âŒ Chatbot em tempo real (<1s esperado)
â†’ +0.8s pode degradar UX
```

---

## ğŸ”¬ Experimentos Recomendados

### 1. Hypothesis Length Optimization
```python
# Testar: 50, 100, 200, 500 tokens
# Medir: Context Precision vs LatÃªncia
# HipÃ³tese: 100-200 tokens = sweet spot
```

### 2. Temperature Impact
```python
# Testar: 0.0, 0.3, 0.7, 1.0
# Medir: Diversity vs Hallucination
# HipÃ³tese: 0.5-0.7 balanceia criatividade
```

### 3. Multi-Hypothesis Ensemble
```python
# Gerar 3 hipÃ³teses diferentes
# Buscar com cada uma
# FusÃ£o dos resultados (RRF)
# Medir: Recall improvement
```

### 4. Prompt Engineering Impact
```python
# Testar:
# - "Responda objetivamente"
# - "Gere resposta tÃ©cnica detalhada"
# - "Use termos do domÃ­nio {domain}"
# Medir: Precision delta
```

---

## ğŸ’» Estrutura de CÃ³digo

```python
# hyde.py

class HyDERAG:
    """
    HyDE: Busca com documento hipotÃ©tico ao invÃ©s da query.

    Pipeline:
    1. Gerar resposta hipotÃ©tica (LLM)
    2. Embed hipÃ³tese
    3. Similarity search
    4. LLM generation final
    """

    def __init__(self, pinecone_index, embeddings, llm):
        self.index = pinecone_index
        self.embeddings = embeddings
        self.llm = llm
        self.k = 5

    def generate_hypothesis(self, query: str) -> str:
        """
        Gera resposta hipotÃ©tica detalhada.
        """
        prompt = f"""
VocÃª Ã© um assistente especializado.

Pergunta do usuÃ¡rio: {query}

Gere uma resposta hipotÃ©tica DETALHADA (100-200 palavras)
usando termos tÃ©cnicos e especÃ­ficos do domÃ­nio.

NÃƒO diga "nÃ£o sei" ou "preciso verificar".
Seja especÃ­fico e declarativo, mesmo que a resposta seja uma estimativa.

Resposta hipotÃ©tica:
"""
        response = self.llm.invoke(
            prompt,
            temperature=0.7,  # Criatividade para preencher lacunas
            max_tokens=200
        )
        return response.content

    def retrieve(self, query: str) -> List[Document]:
        """
        Busca usando embedding da HIPÃ“TESE.
        """
        # Gerar hipÃ³tese
        hypothesis = self.generate_hypothesis(query)

        # Embed hipÃ³tese (nÃ£o query original)
        hyp_vector = self.embeddings.embed_query(hypothesis)

        # Busca
        results = self.index.query(
            vector=hyp_vector,
            top_k=self.k,
            include_metadata=True
        )

        return self._parse_results(results), hypothesis

    def generate(self, query: str, context: List[Document]) -> str:
        """
        GeraÃ§Ã£o final com query ORIGINAL (nÃ£o hipÃ³tese).
        """
        prompt = self._build_prompt(query, context)
        response = self.llm.invoke(prompt, temperature=0.0)
        return response.content

    def query(self, query: str) -> Dict:
        """
        Pipeline completo com mÃ©tricas.
        """
        start_time = time.time()

        # Retrieve com HyDE
        chunks, hypothesis = self.retrieve(query)

        # Generate
        response = self.generate(query, chunks)

        latency = time.time() - start_time

        return {
            "response": response,
            "chunks": chunks,
            "hypothesis": hypothesis,  # Para debug
            "metrics": {
                "latency": latency,
                "chunks_used": len(chunks),
                "technique": "hyde",
                "hypothesis_length": len(hypothesis.split())
            }
        }
```

---

## ğŸ“ VariaÃ§Ãµes AvanÃ§adas

### 1. Multi-HyDE
```python
# Gerar 3 hipÃ³teses diferentes
hyp1 = generate_hypothesis(query, perspective="tÃ©cnica")
hyp2 = generate_hypothesis(query, perspective="negÃ³cio")
hyp3 = generate_hypothesis(query, perspective="usuÃ¡rio")

# Buscar com cada uma
chunks1 = search(hyp1, k=10)
chunks2 = search(hyp2, k=10)
chunks3 = search(hyp3, k=10)

# FusÃ£o (Reciprocal Rank Fusion)
final_chunks = rrf_fusion([chunks1, chunks2, chunks3], k=5)
```

**BenefÃ­cio**: +10-15% Recall

---

### 2. HyDE Condicional
```python
def smart_hyde(query):
    """
    Usa HyDE apenas quando necessÃ¡rio.
    """
    # Classificar query
    if is_factual_lookup(query):
        return baseline_rag(query)  # RÃ¡pido
    elif is_complex_exploratory(query):
        return hyde_rag(query)      # PrecisÃ£o
    else:
        return baseline_rag(query)  # Default
```

**BenefÃ­cio**: Reduz custo em 40-60%

---

### 3. HyDE + Reranking
```python
# 1. HyDE retrieval (top-50)
chunks = hyde.retrieve(query, k=50)

# 2. Cross-encoder reranking (top-5)
final_chunks = reranker.rerank(query, chunks, k=5)

# 3. Generation
response = llm.generate(query, final_chunks)
```

**BenefÃ­cio**: Precision chega a 0.95

---

## ğŸ“š ReferÃªncias

**Paper Original:**
- Gao et al. (2022) - "Precise Zero-Shot Dense Retrieval without Relevance Labels"
- arXiv:2212.10496

**ImplementaÃ§Ãµes:**
- LangChain: `HypotheticalDocumentEmbedder`
- LlamaIndex: `HyDEQueryTransform`

**Benchmarks:**
- BEIR dataset: +12% nDCG@10 vs BM25
- Natural Questions: +8% vs DPR

---

## ğŸ¯ Aprendizados Chave

1. **Resposta > Pergunta**: Documentos sÃ£o declarativos, nÃ£o interrogativos
2. **Trade-off Custo vs PrecisÃ£o**: 2x custo para +20% precisÃ£o (vale a pena?)
3. **Prompt Engineering CrÃ­tico**: HipÃ³tese genÃ©rica = desperdÃ­cio
4. **NÃ£o Ã© Silver Bullet**: Lookup simples nÃ£o se beneficia
5. **Combina bem**: HyDE + Reranking = excelente

---

## ğŸ“ˆ ProgressÃ£o de Complexidade

```
Baseline RAG
    â†“
HyDE (vocÃª estÃ¡ aqui)
    â†“
    â”œâ”€â†’ Multi-HyDE (mÃºltiplas perspectivas)
    â”œâ”€â†’ HyDE + Reranking (mÃ¡xima precisÃ£o)
    â””â”€â†’ Conditional HyDE (otimizaÃ§Ã£o custo)
```

---

**TÃ©cnica Anterior**: [Baseline RAG](./BASELINE_RAG.md)
**PrÃ³xima TÃ©cnica**: [Reranking](./RERANKING.md)
