# Graph RAG - Retrieval Aumentado com Grafos de Conhecimento

## üìã Defini√ß√£o

**Graph RAG** combina **RAG tradicional com Grafos de Conhecimento** (Knowledge Graphs) para capturar e explorar **rela√ß√µes estruturadas entre entidades**.

Ao inv√©s de apenas buscar chunks de texto similares, Graph RAG navega por **conex√µes sem√¢nticas** entre entidades (pessoas, lugares, conceitos, eventos).

**Insight**: Conhecimento n√£o √© s√≥ texto isolado. √â uma rede de entidades e rela√ß√µes. Graph RAG explora essa estrutura.

---

## üîÑ Como Funciona

### Pipeline Completo

```
1. INDEXA√á√ÉO (Setup - Mais Complexo)
   ‚îú‚îÄ Carregar documentos
   ‚îú‚îÄ Extrair entidades (NER - Named Entity Recognition)
   ‚îÇ  ‚îî‚îÄ Pessoas, Organiza√ß√µes, Locais, Datas, Conceitos
   ‚îú‚îÄ Extrair rela√ß√µes entre entidades
   ‚îÇ  ‚îî‚îÄ "Jo√£o Silva [TRABALHA_EM] XYZ Corp"
   ‚îÇ  ‚îî‚îÄ "XYZ Corp [LOCALIZADA_EM] S√£o Paulo"
   ‚îú‚îÄ Construir Grafo de Conhecimento (Neo4j)
   ‚îÇ  ‚îî‚îÄ N√≥s = Entidades
   ‚îÇ  ‚îî‚îÄ Arestas = Rela√ß√µes
   ‚îî‚îÄ Armazenar chunks originais no Vector DB

2. HYBRID RETRIEVAL (Runtime)
   ‚îú‚îÄ Query: "Experi√™ncia do CFO da XYZ Corp"
   ‚îú‚îÄ (A) Vector Search: Busca chunks similares
   ‚îÇ  ‚îî‚îÄ "CFO XYZ Corp: Jo√£o Silva..."
   ‚îú‚îÄ (B) Graph Traversal: Navega rela√ß√µes
   ‚îÇ  ‚îî‚îÄ Encontra "Jo√£o Silva" no grafo
   ‚îÇ  ‚îî‚îÄ Segue arestas [TRABALHOU_EM], [FORMADO_EM]
   ‚îÇ  ‚îî‚îÄ "Jo√£o Silva ‚Üí Goldman Sachs (10 anos)"
   ‚îÇ  ‚îî‚îÄ "Jo√£o Silva ‚Üí Harvard MBA"
   ‚îî‚îÄ Combinar chunks (Vector + Graph)

3. GERA√á√ÉO
   ‚îú‚îÄ Prompt com contexto h√≠brido
   ‚îú‚îÄ LLM sintetiza resposta
   ‚îî‚îÄ Resposta com informa√ß√µes conectadas
```

### Compara√ß√£o Visual

**Baseline RAG:**
```
Query ‚Üí Vector Search ‚Üí Chunks isolados
```

**Graph RAG:**
```
Query
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Vector DB   ‚îÇ Knowledge Graph ‚îÇ
‚îÇ (chunks)    ‚îÇ (rela√ß√µes)      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Chunk 1     ‚îÇ Jo√£o Silva      ‚îÇ
‚îÇ Chunk 2     ‚îÇ   ‚Üì [CFO_DE]    ‚îÇ
‚îÇ Chunk 3     ‚îÇ XYZ Corp        ‚îÇ
‚îÇ             ‚îÇ   ‚Üì [TRABALHOU] ‚îÇ
‚îÇ             ‚îÇ Goldman Sachs   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì [Combina contexto]
LLM ‚Üí Resposta rica em conex√µes
```

---

## üí° Por Que Funciona?

### Problema do RAG Tradicional

```python
Query: "Qual a experi√™ncia anterior do CFO da XYZ Corp?"

# Vector Search (chunks isolados):
Chunk 1: "CFO da XYZ Corp √© Jo√£o Silva"           ‚úÖ (quem)
Chunk 2: "Jo√£o Silva graduado em Harvard"        ‚úÖ (educa√ß√£o)
Chunk 3: "Goldman Sachs contratou executivos"    ‚ùå (n√£o conecta!)
Chunk 4: "Experi√™ncia em finan√ßas corporativas"  ‚ùå (gen√©rico)

# ‚ùå Chunks N√ÉO conectam "Jo√£o Silva" ‚Üí "Goldman Sachs"
# ‚ùå Informa√ß√£o est√° fragmentada
```

### Solu√ß√£o Graph RAG

```python
# Knowledge Graph (estruturado):

(Jo√£o Silva)-[CFO_DE]->(XYZ Corp)
(Jo√£o Silva)-[TRABALHOU_EM {anos: 10}]->(Goldman Sachs)
(Jo√£o Silva)-[FORMADO_EM]->(Harvard)
(Goldman Sachs)-[TIPO]->(Banco de Investimento)

# Query executada:
MATCH (cfo)-[CFO_DE]->(empresa {nome: "XYZ Corp"})
MATCH (cfo)-[TRABALHOU_EM]->(experiencia_anterior)
RETURN cfo, experiencia_anterior

# Resultado:
{
  "cfo": "Jo√£o Silva",
  "experiencias": [
    {"empresa": "Goldman Sachs", "anos": 10, "cargo": "VP Finan√ßas"}
  ],
  "educacao": [
    {"instituicao": "Harvard", "grau": "MBA"}
  ]
}

# ‚úÖ Informa√ß√£o CONECTADA e ESTRUTURADA
```

---

## üî¨ Exemplo Pr√°tico Detalhado

### Caso 1: Multi-Hop Query

**Query:**
```
"Quem s√£o os colegas de Jo√£o Silva que tamb√©m trabalharam no Goldman Sachs?"
```

**Baseline RAG (Imposs√≠vel):**
```python
# Vector search n√£o consegue resolver:
# 1. Identificar colegas de Jo√£o Silva
# 2. Filtrar s√≥ os que passaram por Goldman Sachs
# 3. Conectar as duas condi√ß√µes

‚ùå Retorna chunks gen√©ricos sobre "colegas" ou "Goldman Sachs"
```

**Graph RAG (Resolve):**
```cypher
// Cypher query (Neo4j)
MATCH (joao:Pessoa {nome: "Jo√£o Silva"})-[:TRABALHA_COM]->(colega:Pessoa)
MATCH (colega)-[:TRABALHOU_EM]->(gs:Empresa {nome: "Goldman Sachs"})
RETURN colega.nome, colega.cargo

// Resultado:
[
  {"nome": "Maria Santos", "cargo": "COO"},
  {"nome": "Pedro Lima", "cargo": "CTO"}
]

// Chunks adicionais buscados:
- Bio de Maria Santos
- Bio de Pedro Lima
```

**Resposta Final:**
```
"Jo√£o Silva tem dois colegas que tamb√©m trabalharam no Goldman Sachs:
Maria Santos (atual COO) trabalhou l√° por 8 anos como Diretora...
Pedro Lima (atual CTO) foi VP de Tecnologia..."
```

---

### Caso 2: Infer√™ncia de Rela√ß√µes

**Query:**
```
"Produtos da empresa que Jo√£o Silva fundou antes de entrar na XYZ Corp"
```

**Graph Traversal:**
```cypher
MATCH (joao:Pessoa {nome: "Jo√£o Silva"})-[:FUNDOU]->(empresa_anterior:Empresa)
MATCH (empresa_anterior)-[:OFERECE]->(produto:Produto)
WHERE joao.entrada_xyz > empresa_anterior.fundacao
RETURN empresa_anterior.nome, produto.nome

// Resultado:
{
  "empresa": "TechStart Inc",
  "produtos": ["CloudSync", "DataFlow"]
}
```

**Vector RAG Sozinho:**
```
‚ùå N√£o consegue inferir timeline
‚ùå N√£o conecta funda√ß√£o ‚Üí entrada XYZ ‚Üí produtos
```

---

## ‚öôÔ∏è Configura√ß√£o Padr√£o

| Par√¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Graph DB** | Neo4j | Mais maduro para KG |
| **Entity Extraction** | spaCy + LLM | Precis√£o NER |
| **Relation Extraction** | GPT-4 / Gemini | Complexo, precisa LLM |
| **Vector DB** | Pinecone | Chunks textuais |
| **Hybrid Weight** | 0.5 Vector + 0.5 Graph | Balance |
| **Max Graph Depth** | 2-3 hops | Evita explos√£o combinat√≥ria |

### Tipos de Entidades Comuns

| Tipo | Exemplos | Uso |
|------|----------|-----|
| **PESSOA** | Jo√£o Silva, Maria Santos | Stakeholders |
| **EMPRESA** | XYZ Corp, Goldman Sachs | Organiza√ß√µes |
| **LOCAL** | S√£o Paulo, Nova York | Geografia |
| **PRODUTO** | CloudSync, iPhone | Ofertas |
| **EVENTO** | IPO, Fus√£o, Lan√ßamento | Marcos temporais |
| **CONCEITO** | Machine Learning, ROI | Dom√≠nios |

### Tipos de Rela√ß√µes Comuns

| Rela√ß√£o | Exemplo | Dire√ß√£o |
|---------|---------|---------|
| **TRABALHA_EM** | Pessoa ‚Üí Empresa | Directed |
| **TRABALHOU_EM** | Pessoa ‚Üí Empresa (passado) | Directed |
| **CFO_DE / CEO_DE** | Pessoa ‚Üí Empresa | Directed |
| **REPORTA_PARA** | Pessoa ‚Üí Pessoa | Directed |
| **OFERECE** | Empresa ‚Üí Produto | Directed |
| **LOCALIZADA_EM** | Empresa ‚Üí Local | Directed |
| **RELACIONADO_A** | Conceito ‚Üî Conceito | Undirected |

---

## ‚úÖ Vantagens

### 1. Multi-Hop Queries Nativas
```
‚úÖ "Amigos de amigos que trabalham em tech"
‚úÖ "Produtos de empresas fundadas por ex-alunos de Harvard"
‚úÖ Queries com 3-4 n√≠veis de conex√£o
```

### 2. Estrutura Expl√≠cita
```
Rela√ß√µes s√£o EXPL√çCITAS, n√£o inferidas:
- "Jo√£o [TRABALHOU_EM] Goldman" (fato)
- N√£o precisa LLM adivinhar conex√£o
‚Üí Reduz alucina√ß√£o
```

### 3. Racioc√≠nio Temporal
```
Grafo armazena timestamps:
- TRABALHOU_EM {inicio: 2010, fim: 2020}
- FUNDOU {data: 2022}

Query: "O que Jo√£o fez ANTES de fundar empresa?"
‚Üí Filtro temporal nativo
```

### 4. Agrega√ß√µes Complexas
```cypher
// Quantos funcion√°rios da XYZ vieram do Goldman?
MATCH (p:Pessoa)-[:TRABALHOU_EM]->(gs {nome: "Goldman Sachs"})
MATCH (p)-[:TRABALHA_EM]->(xyz {nome: "XYZ Corp"})
RETURN count(p)

// ‚úÖ Agrega√ß√£o imposs√≠vel em Vector RAG
```

### 5. Explorabilidade
```
User pode "navegar" o grafo:
- Visualizar conex√µes
- Descobrir padr√µes inesperados
- Insights emergentes
```

### 6. Precis√£o Factual
```
Fatos estruturados = menos alucina√ß√£o
"Jo√£o Silva √â CFO" (grafo)
vs
"Jo√£o Silva parece ser CFO" (LLM inferindo de chunk)
```

---

## ‚ùå Desvantagens

### 1. Complexidade de Setup Extrema
```
Baseline: Carregar docs ‚Üí Embed ‚Üí Pinecone (30 min)

Graph RAG:
1. Carregar docs (5 min)
2. NER - extrair entidades (2h com LLM)
3. Relation extraction (4h com LLM)
4. Construir grafo no Neo4j (1h)
5. Validar/limpar rela√ß√µes (2h manual)
6. Integrar Vector + Graph (1h c√≥digo)

Total: 8-10 horas vs 30 minutos
```

### 2. Custo de Indexa√ß√£o Massivo
```
# Relation Extraction com LLM:
- 1000 documentos
- ~500 par√°grafos/doc = 500K par√°grafos
- GPT-4: $0.01/1K tokens
- Avg 200 tokens/par√°grafo

Custo: 500K √ó 0.2K √ó $0.01/1K = $1,000+ APENAS indexa√ß√£o
Baseline: ~$5-10
```

### 3. Manuten√ß√£o do Grafo
```
Documentos atualizam:
- "Jo√£o Silva agora √© CEO (n√£o mais CFO)"

‚ùå Precisa:
1. Detectar mudan√ßa
2. Atualizar rela√ß√µes no grafo
3. Manter consist√™ncia

Grafo pode ficar desatualizado rapidamente
```

### 4. Lat√™ncia em Queries Complexas
```
Cypher query com 3-4 hops:
MATCH (a)-[r1]->(b)-[r2]->(c)-[r3]->(d)

Em grafo grande (>100K n√≥s):
- Pode levar 2-5 segundos APENAS no grafo
- + Vector search (1s)
- + LLM generation (1s)

Total: 4-7 segundos
```

### 5. Erros em Entity/Relation Extraction
```
NER pode errar:
"Apple lan√ßou iPhone" ‚Üí
  ‚ùå "Apple" detectado como [FRUTA], n√£o [EMPRESA]

Relation extraction pode errar:
"Jo√£o trabalhou COM Maria" ‚Üí
  ‚ùå Extrai: Jo√£o [TRABALHOU_EM] Maria (rela√ß√£o errada!)

Erros propagam pelo grafo ‚Üí Respostas incorretas
```

### 6. N√£o Funciona para Todo Dom√≠nio
```
‚úÖ Bom para: Conhecimento factual estruturado
   - Org charts, rela√ß√µes corporativas
   - Hist√≥ria, genealogia
   - Redes sociais profissionais

‚ùå Ruim para: Conhecimento conceitual abstrato
   - "Como funciona machine learning?"
   - Tutoriais, guias
   - Documenta√ß√£o t√©cnica
```

### 7. Escalabilidade
```
Neo4j performance degrada com:
- >10M n√≥s
- >100M arestas
- Queries multi-hop em grafo denso

Precisa sharding, otimiza√ß√µes complexas
```

---

## üìä M√©tricas Esperadas

### RAGAS Scores vs Baseline

| M√©trica | Baseline | Graph RAG | Œî | Contexto |
|---------|----------|-----------|---|----------|
| **Faithfulness** | 0.75-0.85 | 0.90-0.98 | +15-20% | Fatos estruturados |
| **Answer Relevancy** | 0.70-0.85 | 0.80-0.92 | +10-15% | Conex√µes expl√≠citas |
| **Context Precision** | 0.60-0.75 | 0.75-0.88 | +15-20% | Rela√ß√µes filtradas |
| **Context Recall** | 0.50-0.70 | 0.70-0.90 | +30-40% | Multi-hop retrieval |

**Nota**: M√©tricas APENAS para queries que se beneficiam de grafo (multi-hop, rela√ß√µes).

### Performance

| M√©trica | Baseline | Graph RAG |
|---------|----------|-----------|
| **Lat√™ncia (query simples)** | 1.2-2.5s | 2.0-4.0s |
| **Lat√™ncia (multi-hop)** | N/A | 3.0-7.0s |
| **Custo/Query** | $0.001-0.003 | $0.002-0.005 |
| **Indexa√ß√£o (1K docs)** | $5-10 | $500-1500 |
| **Indexa√ß√£o (tempo)** | 30 min | 6-10 horas |

---

## üéØ Quando Usar Graph RAG

### ‚úÖ Casos Ideais

**1. Queries Multi-Hop e Relacionais**
```
‚úÖ "Quem s√£o colegas de X que trabalharam em Y?"
‚úÖ "Produtos de empresas fundadas por ex-funcion√°rios"
‚úÖ "Cadeia de comando: quem reporta para quem?"
```

**2. Conhecimento Factual Estruturado**
```
‚úÖ Org charts corporativos
‚úÖ Genealogia / √°rvores familiares
‚úÖ Redes de cita√ß√µes acad√™micas
‚úÖ Supply chain / fornecedores
```

**3. An√°lise de Redes**
```
‚úÖ "Qual a pessoa mais conectada?" (centrality)
‚úÖ "Identificar comunidades/clusters"
‚úÖ "Caminhos mais curtos entre entidades"
```

**4. Racioc√≠nio Temporal**
```
‚úÖ "O que aconteceu ANTES de X?"
‚úÖ "Sequ√™ncia de eventos que levaram a Y"
‚úÖ "Timeline de carreira de uma pessoa"
```

**5. Dom√≠nios com Relacionamentos Ricos**
```
‚úÖ Legal (jurisprud√™ncia, cita√ß√µes de leis)
‚úÖ Biomedicina (prote√≠nas, genes, doen√ßas)
‚úÖ Finan√ßas (investimentos, acionistas)
```

---

### ‚ùå Quando N√ÉO Usar

**1. Conhecimento N√£o-Estruturado**
```
‚ùå Tutoriais "como fazer"
‚ùå Documenta√ß√£o t√©cnica conceitual
‚ùå Literatura, artigos opinativos
‚Üí Use Baseline ou HyDE
```

**2. Budget Limitado**
```
‚ùå Indexa√ß√£o custa 100-300x baseline
‚ùå Precisa time de engenharia dedicado
‚ùå Manuten√ß√£o cont√≠nua do grafo
```

**3. Dados Vol√°teis**
```
‚ùå Informa√ß√£o muda diariamente
‚ùå Atualiza√ß√£o de grafo = custosa
‚Üí Use Vector RAG (re-index mais f√°cil)
```

**4. Queries Simples**
```
‚ùå Lookup direto ("Qual o telefone?")
‚ùå FAQs b√°sicas
‚Üí Overhead de grafo n√£o compensa
```

**5. Sem Infraestrutura**
```
‚ùå Neo4j = complexo (vs Pinecone SaaS simples)
‚ùå Precisa expertise em Cypher
‚ùå Monitoramento de DB adicional
```

**6. Prototipagem R√°pida**
```
‚ùå Precisa validar ideia em 1 semana
‚ùå MVP com 0 custo
‚Üí Comece com Baseline, adicione Graph depois
```

---

## üî¨ Experimentos Recomendados

### 1. Hybrid Weight Tuning
```python
# Testar: 0.3 vector + 0.7 graph, 0.5/0.5, 0.7/0.3
# Medir: Precision vs Recall
# Hip√≥tese: Depende do tipo de query
```

### 2. Graph Depth Limit
```cypher
# Testar: max_depth = 1, 2, 3, 4
# Medir: Recall vs Lat√™ncia
# Hip√≥tese: depth=2 suficiente para 90% queries
```

### 3. Entity Extraction Quality
```python
# Comparar:
# - spaCy (r√°pido, impreciso)
# - GPT-4 (lento, preciso)
# - Fine-tuned BERT (balance)
# Medir: F1 de entidades vs Custo
```

### 4. Relation Extraction Prompting
```python
# Testar diferentes prompts para LLM
# Medir: Precis√£o de rela√ß√µes extra√≠das
# Valida√ß√£o manual em sample de 100 rela√ß√µes
```

---

## üíª Estrutura de C√≥digo

```python
# graph_rag.py

from neo4j import GraphDatabase
import spacy

class GraphRAG:
    """
    RAG com Knowledge Graph para queries relacionais.

    Pipeline:
    1. Hybrid retrieval (Vector + Graph)
    2. Merge contextos
    3. LLM generation
    """

    def __init__(self, pinecone_index, embeddings, llm, neo4j_uri, neo4j_user, neo4j_password):
        # Vector DB
        self.index = pinecone_index
        self.embeddings = embeddings
        self.llm = llm

        # Graph DB
        self.graph_driver = GraphDatabase.driver(
            neo4j_uri,
            auth=(neo4j_user, neo4j_password)
        )

        # NER
        self.nlp = spacy.load("pt_core_news_lg")

        self.k_vector = 5
        self.k_graph = 5
        self.hybrid_weight = 0.5  # 50% vector, 50% graph

    def extract_entities(self, query: str) -> List[str]:
        """
        Extrai entidades da query (NER).
        """
        doc = self.nlp(query)
        entities = [ent.text for ent in doc.ents]
        return entities

    def vector_search(self, query: str, k: int) -> List[Document]:
        """
        Busca vetorial tradicional.
        """
        query_vector = self.embeddings.embed_query(query)

        results = self.index.query(
            vector=query_vector,
            top_k=k,
            include_metadata=True
        )

        return self._parse_results(results)

    def graph_search(self, entities: List[str], query: str) -> List[Dict]:
        """
        Busca no grafo de conhecimento.
        """
        if not entities:
            return []

        # Cypher query din√¢mica
        cypher = f"""
        MATCH (e:Entity)
        WHERE e.name IN {entities}
        OPTIONAL MATCH (e)-[r]-(connected:Entity)
        RETURN e, r, connected
        LIMIT {self.k_graph}
        """

        with self.graph_driver.session() as session:
            result = session.run(cypher)
            graph_data = []

            for record in result:
                entity = record.get("e")
                relation = record.get("r")
                connected = record.get("connected")

                if entity and connected and relation:
                    graph_data.append({
                        "entity": dict(entity),
                        "relation": type(relation).__name__,
                        "connected": dict(connected)
                    })

        return graph_data

    def graph_to_text(self, graph_data: List[Dict]) -> str:
        """
        Converte dados do grafo em texto para LLM.
        """
        text_parts = []

        for item in graph_data:
            entity = item["entity"]["name"]
            relation = item["relation"]
            connected = item["connected"]["name"]

            text_parts.append(
                f"{entity} {relation} {connected}"
            )

        return "\n".join(text_parts)

    def hybrid_retrieval(self, query: str) -> Tuple[List[Document], str]:
        """
        Combina Vector + Graph retrieval.
        """
        # 1. Extrair entidades da query
        entities = self.extract_entities(query)

        # 2. Vector search
        vector_chunks = self.vector_search(query, self.k_vector)

        # 3. Graph search
        graph_data = self.graph_search(entities, query)
        graph_context = self.graph_to_text(graph_data)

        return vector_chunks, graph_context

    def generate(self, query: str, vector_chunks: List[Document], graph_context: str) -> str:
        """
        Gera√ß√£o com contexto h√≠brido.
        """
        # Montar contexto combinado
        context_parts = []

        # Contexto do grafo (rela√ß√µes estruturadas)
        if graph_context:
            context_parts.append("Rela√ß√µes conhecidas:")
            context_parts.append(graph_context)
            context_parts.append("")

        # Contexto vetorial (chunks de texto)
        context_parts.append("Informa√ß√µes adicionais:")
        for chunk in vector_chunks:
            context_parts.append(chunk.page_content)
            context_parts.append("")

        full_context = "\n".join(context_parts)

        # Prompt
        prompt = f"""
Contexto:
{full_context}

Pergunta: {query}

Responda baseado no contexto acima, priorizando as rela√ß√µes estruturadas.
"""

        response = self.llm.invoke(prompt, temperature=0.0)
        return response.content

    def query(self, query: str) -> Dict:
        """
        Pipeline completo Graph RAG.
        """
        start_time = time.time()

        # 1. Hybrid retrieval
        t1 = time.time()
        vector_chunks, graph_context = self.hybrid_retrieval(query)
        retrieval_time = time.time() - t1

        # 2. Generation
        t2 = time.time()
        response = self.generate(query, vector_chunks, graph_context)
        generation_time = time.time() - t2

        total_latency = time.time() - start_time

        return {
            "response": response,
            "vector_chunks": vector_chunks,
            "graph_context": graph_context,
            "metrics": {
                "latency_total": total_latency,
                "latency_retrieval": retrieval_time,
                "latency_generation": generation_time,
                "chunks_vector": len(vector_chunks),
                "graph_relations": len(graph_context.split('\n')) if graph_context else 0,
                "technique": "graph_rag"
            }
        }

    def close(self):
        """
        Fechar conex√£o com Neo4j.
        """
        self.graph_driver.close()
```

---

## üéì Constru√ß√£o do Knowledge Graph

### Entity Extraction
```python
def extract_entities_with_llm(document: str) -> List[Dict]:
    """
    Extrai entidades usando LLM.
    """
    prompt = f"""
Analise o texto abaixo e extraia todas as entidades.

Para cada entidade, identifique:
- Nome
- Tipo (PESSOA, EMPRESA, LOCAL, PRODUTO, EVENTO, CONCEITO)

Texto:
{document}

Retorne JSON:
[{{"nome": "...", "tipo": "..."}}]
"""

    response = llm.invoke(prompt)
    entities = json.loads(response.content)
    return entities
```

### Relation Extraction
```python
def extract_relations_with_llm(document: str, entities: List[Dict]) -> List[Dict]:
    """
    Extrai rela√ß√µes entre entidades.
    """
    entities_str = ", ".join([e["nome"] for e in entities])

    prompt = f"""
Identifique rela√ß√µes entre estas entidades no texto:
{entities_str}

Texto:
{document}

Para cada rela√ß√£o, especifique:
- entidade_origem
- tipo_relacao (TRABALHA_EM, CEO_DE, FUNDOU, etc)
- entidade_destino

Retorne JSON:
[{{"origem": "...", "relacao": "...", "destino": "..."}}]
"""

    response = llm.invoke(prompt)
    relations = json.loads(response.content)
    return relations
```

### Graph Construction
```python
def build_knowledge_graph(documents: List[str]):
    """
    Constr√≥i grafo de conhecimento.
    """
    graph = GraphDatabase.driver(neo4j_uri, auth=(user, password))

    for doc in documents:
        # 1. Extrair entidades
        entities = extract_entities_with_llm(doc)

        # 2. Criar n√≥s no grafo
        with graph.session() as session:
            for entity in entities:
                session.run(
                    "MERGE (e:Entity {name: $name, type: $type})",
                    name=entity["nome"],
                    type=entity["tipo"]
                )

        # 3. Extrair rela√ß√µes
        relations = extract_relations_with_llm(doc, entities)

        # 4. Criar arestas no grafo
        with graph.session() as session:
            for rel in relations:
                session.run(f"""
                    MATCH (origem:Entity {{name: $origem}})
                    MATCH (destino:Entity {{name: $destino}})
                    MERGE (origem)-[r:{rel["relacao"]}]->(destino)
                """,
                    origem=rel["origem"],
                    destino=rel["destino"]
                )
```

---

## üìö Refer√™ncias

**Papers:**
- Microsoft GraphRAG (2024) - "From Local to Global: A Graph RAG Approach"
- Yasunaga et al. (2022) - "Deep Bidirectional Language-Knowledge Graph Pretraining"

**Frameworks:**
- Neo4j + LangChain integration
- LlamaIndex Knowledge Graph Index
- Microsoft GraphRAG (open source)

**Benchmarks:**
- WebQSP (multi-hop): +22% accuracy vs RAG
- ComplexWebQuestions: +15% F1

---

## üéØ Aprendizados Chave

1. **Estrutura > Texto**: Rela√ß√µes expl√≠citas reduzem alucina√ß√£o
2. **Multi-Hop Nativo**: Grafo resolve queries imposs√≠veis para RAG
3. **Trade-off Brutal**: 100x custo de setup para 20-30% melhoria
4. **Dom√≠nio-Espec√≠fico**: N√£o √© silver bullet, funciona para conhecimento factual
5. **Hybrid √© Essential**: Graph sozinho = incompleto. Precisa Vector tamb√©m

---

## üìà Progress√£o de Complexidade

```
Baseline RAG (chunks isolados)
    ‚Üì
Sub-Query (m√∫ltiplas buscas)
    ‚Üì
Fusion (m√∫ltiplas estrat√©gias)
    ‚Üì
Graph RAG (voc√™ est√° aqui) = Conhecimento estruturado
    ‚Üì
Microsoft GraphRAG (global + local communities)
```

---

**T√©cnica Anterior**: [Fusion](./FUSION.md)
**Resumo Comparativo**: [COMPARISON.md](./COMPARISON.md) *(pr√≥ximo)*
