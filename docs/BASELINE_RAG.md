# Baseline RAG - RAG Tradicional

## ğŸ“‹ DefiniÃ§Ã£o

**Baseline RAG** Ã© a implementaÃ§Ã£o mais simples e direta de Retrieval-Augmented Generation. Representa o padrÃ£o "vanilla" sem otimizaÃ§Ãµes avanÃ§adas.

Ã‰ o ponto de partida fundamental para comparar todas as outras tÃ©cnicas.

---

## ğŸ”„ Como Funciona

### Pipeline Completo

```
1. INDEXAÃ‡ÃƒO (Setup - executa 1 vez)
   â”œâ”€ Carregar documentos (PDFs, TXTs, etc)
   â”œâ”€ Dividir em chunks (512-1024 tokens)
   â”œâ”€ Gerar embeddings (text-embedding-004)
   â””â”€ Armazenar no Pinecone

2. RECUPERAÃ‡ÃƒO (Runtime - cada query)
   â”œâ”€ User pergunta: "Qual o lucro Q3?"
   â”œâ”€ Gerar embedding da pergunta
   â”œâ”€ Busca de similaridade no Pinecone
   â””â”€ Retornar top-k chunks (k=5)

3. GERAÃ‡ÃƒO (Runtime)
   â”œâ”€ Montar prompt: [System] + [Chunks] + [Query]
   â”œâ”€ Enviar para LLM (Gemini 2.5 Flash)
   â””â”€ Retornar resposta final
```

### Exemplo PrÃ¡tico

**Input:**
```
Query: "Qual foi o lucro do terceiro trimestre?"
```

**Processo:**
```python
# 1. Embedding da query
query_vector = embeddings.embed_query("Qual foi o lucro do terceiro trimestre?")
# â†’ [0.023, -0.145, 0.891, ...] (768 dimensÃµes)

# 2. Busca vetorial
results = pinecone.query(
    vector=query_vector,
    top_k=5,
    include_metadata=True
)
# â†’ Retorna 5 chunks mais similares

# 3. Construir prompt
prompt = f"""
Contexto:
{chunk_1.text}
{chunk_2.text}
{chunk_3.text}
{chunk_4.text}
{chunk_5.text}

Pergunta: {query}

Responda baseado APENAS no contexto acima.
"""

# 4. Gerar resposta
response = llm.invoke(prompt)
```

**Output:**
```
"O lucro lÃ­quido do terceiro trimestre foi de R$ 3 bilhÃµes,
representando um crescimento de 15% em relaÃ§Ã£o ao Q2."
```

---

## âš™ï¸ ConfiguraÃ§Ã£o PadrÃ£o

| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Chunk Size** | 512 tokens | Balance entre contexto e granularidade |
| **Chunk Overlap** | 50 tokens | Evita perder contexto nas bordas |
| **Top-K** | 5 chunks | Suficiente para maioria das perguntas |
| **Embedding Model** | text-embedding-004 | Google - gratuito e eficiente |
| **LLM** | Gemini 1.5 Flash | Custo-benefÃ­cio ideal |
| **Temperature** | 0.0 | Respostas determinÃ­sticas |

---

## âœ… Vantagens

### 1. Simplicidade
- **ImplementaÃ§Ã£o**: 50-100 linhas de cÃ³digo
- **ManutenÃ§Ã£o**: Arquitetura linear, fÃ¡cil debug
- **Onboarding**: Equipe entende rapidamente

### 2. Velocidade
- **LatÃªncia**: ~1-2 segundos total
- **Single retrieval step**: Sem mÃºltiplas chamadas ao Vector DB
- **Direto ao ponto**: Sem processamento intermediÃ¡rio

### 3. Custo-Efetivo
- **Embeddings**: 1 geraÃ§Ã£o por query
- **LLM**: 1 chamada Ãºnica
- **Vector DB**: 1 busca simples
- **Estimativa**: $0.001-0.003 por query

### 4. Previsibilidade
- **Comportamento**: DeterminÃ­stico (temp=0)
- **Debugging**: FÃ¡cil rastrear erros
- **MÃ©tricas**: Baseline estÃ¡vel para comparaÃ§Ã£o

### 5. Suficiente para Casos Simples
- **FAQs**: Perguntas diretas e objetivas
- **Lookup**: "Qual o telefone?", "Quem Ã© o CEO?"
- **DocumentaÃ§Ã£o**: Busca em manuais tÃ©cnicos

---

## âŒ Desvantagens

### 1. Queries AmbÃ­guas
**Problema**: Embedding da query pode nÃ£o capturar intenÃ§Ã£o real

```
Query: "Como melhorar performance?"
â†’ Pode recuperar chunks sobre:
  - Performance de vendas
  - Performance tÃ©cnica (cÃ³digo)
  - Performance financeira

âŒ Sem contexto, nÃ£o sabe qual o usuÃ¡rio quer
```

**Impacto**: Context Precision baixo (~0.6-0.7)

---

### 2. Queries Complexas Multi-Hop
**Problema**: Precisa de informaÃ§Ã£o de mÃºltiplos documentos correlacionados

```
Query: "Compare lucro Q3 com investimento em marketing"
â†’ Precisa:
  - Chunk sobre lucro (doc_financeiro.pdf)
  - Chunk sobre marketing (doc_marketing.pdf)

âŒ Busca vetorial pode nÃ£o pegar ambos no top-5
```

**Impacto**: Context Recall baixo (~0.5-0.6)

---

### 3. VocabulÃ¡rio Mismatch
**Problema**: User usa termos diferentes dos documentos

```
Query: "Quanto a empresa faturou?"
Documento usa: "Receita lÃ­quida foi..."

âŒ "faturou" â‰  "receita" semanticamente similar, mas pode nÃ£o ranquear bem
```

**SoluÃ§Ã£o AvanÃ§ada**: HyDE, Query Expansion

---

### 4. Chunks Grandes = RuÃ­do
**Problema**: Chunks de 512 tokens tÃªm muito contexto extra

```
Chunk recuperado:
"...falamos sobre estratÃ©gia de RH, benefÃ­cios, treinamento...
[INFORMAÃ‡ÃƒO ÃšTIL: lucro foi R$ 3bi]
...depois discutimos plano de carreira, retenÃ§Ã£o..."

âŒ LLM pode se distrair com 80% de ruÃ­do
```

**Impacto**: Faithfulness pode cair para 0.7

---

### 5. Sem Reranking
**Problema**: Ordem dos chunks Ã© puramente por similaridade vetorial

```
Top-5 chunks:
1. Score 0.89 â†’ "Lucro preliminar estimado..."  (nÃ£o final)
2. Score 0.87 â†’ "DiscussÃ£o sobre lucro..."      (vago)
3. Score 0.85 â†’ "Lucro Q3: R$ 3 bilhÃµes"        (âœ… MELHOR)
4. Score 0.82 â†’ Irrelevante
5. Score 0.80 â†’ Irrelevante

âŒ Melhor chunk estÃ¡ em 3Âº lugar
```

**SoluÃ§Ã£o AvanÃ§ada**: Reranking com Cross-Encoder

---

### 6. Cold Start Problem
**Problema**: Primeira query sempre Ã© lenta

```
1Âª query: 3-4 segundos (carregar modelo, conectar Pinecone)
2Âª+ query: 1-2 segundos (cache ativo)
```

---

## ğŸ“Š MÃ©tricas Esperadas (Baseline)

### RAGAS Scores TÃ­picos

| MÃ©trica | Score Esperado | InterpretaÃ§Ã£o |
|---------|----------------|---------------|
| **Faithfulness** | 0.75 - 0.85 | Boa fidelidade aos chunks |
| **Answer Relevancy** | 0.70 - 0.85 | Responde a pergunta, mas pode ser genÃ©rico |
| **Context Precision** | 0.60 - 0.75 | ~60% dos chunks sÃ£o Ãºteis |
| **Context Recall** | 0.50 - 0.70 | Perde 30-50% de info necessÃ¡ria |

### Performance

| MÃ©trica | Valor TÃ­pico |
|---------|--------------|
| **LatÃªncia** | 1.2 - 2.5s |
| **Custo/Query** | $0.001 - $0.003 |
| **Throughput** | ~30-50 queries/min |

---

## ğŸ¯ Quando Usar Baseline RAG

### âœ… Casos Ideais

**1. FAQs e DocumentaÃ§Ã£o Simples**
```
- "Qual o horÃ¡rio de atendimento?"
- "Como resetar senha?"
- "PolÃ­tica de devoluÃ§Ã£o"
```

**2. Lookup Direto**
```
- "Quem Ã© o CFO?"
- "EndereÃ§o da matriz"
- "CÃ³digo do produto X"
```

**3. MVP e Prototipagem**
```
- Validar viabilidade de RAG
- DemonstraÃ§Ã£o rÃ¡pida
- Baseline para comparaÃ§Ã£o
```

**4. Baixo Volume de Queries**
```
- <100 queries/dia
- Sem SLA crÃ­tico
- Budget limitado
```

---

### âŒ Quando NÃƒO Usar

**1. Queries AnalÃ­ticas Complexas**
```
âŒ "Analise a correlaÃ§Ã£o entre investimento em P&D e crescimento de receita nos Ãºltimos 3 anos"
â†’ Use: Sub-Query ou Graph RAG
```

**2. DomÃ­nios com JargÃ£o TÃ©cnico**
```
âŒ Medicina, JurÃ­dico (vocabulÃ¡rio muito especÃ­fico)
â†’ Use: HyDE ou Domain-Specific Embeddings
```

**3. Necessidade de Alta PrecisÃ£o**
```
âŒ Compliance, RegulatÃ³rio, Financeiro
â†’ Use: Reranking + Validation
```

**4. Multi-Idioma**
```
âŒ Queries em PT, docs em EN
â†’ Use: Multilingual Embeddings + Query Translation
```

---

## ğŸ”¬ Experimentos Recomendados

### 1. VariaÃ§Ã£o de Top-K
```python
# Testar: k=3, k=5, k=10, k=20
# Medir: Context Recall vs Precision
# HipÃ³tese: k maior = recallâ†‘ mas precisionâ†“
```

### 2. Chunk Size Optimization
```python
# Testar: 256, 512, 1024, 2048 tokens
# Medir: Faithfulness e latÃªncia
# HipÃ³tese: Chunks menores = mais precisos mas recall menor
```

### 3. Overlap Impact
```python
# Testar: 0%, 10%, 25%, 50% overlap
# Medir: Context Recall
# HipÃ³tese: Overlap evita perder informaÃ§Ã£o nas bordas
```

---

## ğŸ’» Estrutura de CÃ³digo

```python
# baseline_rag.py

class BaselineRAG:
    """
    ImplementaÃ§Ã£o RAG tradicional sem otimizaÃ§Ãµes.

    Pipeline:
    1. Embed query
    2. Similarity search (top-k)
    3. LLM generation
    """

    def __init__(self, pinecone_index, embeddings, llm):
        self.index = pinecone_index
        self.embeddings = embeddings
        self.llm = llm
        self.k = 5  # top-k chunks

    def retrieve(self, query: str) -> List[Document]:
        """Busca vetorial simples"""
        query_vector = self.embeddings.embed_query(query)
        results = self.index.query(
            vector=query_vector,
            top_k=self.k,
            include_metadata=True
        )
        return self._parse_results(results)

    def generate(self, query: str, context: List[Document]) -> str:
        """GeraÃ§Ã£o com LLM"""
        prompt = self._build_prompt(query, context)
        response = self.llm.invoke(prompt)
        return response.content

    def query(self, query: str) -> Dict:
        """Pipeline completo com mÃ©tricas"""
        start_time = time.time()

        # Retrieve
        chunks = self.retrieve(query)

        # Generate
        response = self.generate(query, chunks)

        latency = time.time() - start_time

        return {
            "response": response,
            "chunks": chunks,
            "metrics": {
                "latency": latency,
                "chunks_used": len(chunks),
                "technique": "baseline"
            }
        }
```

---

## ğŸ“š ReferÃªncias

**Papers:**
- Lewis et al. (2020) - "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- Original RAG paper from Meta AI

**Benchmarks:**
- Natural Questions (NQ)
- TriviaQA
- RAGAS evaluation framework

---

## ğŸ“ Aprendizados Chave

1. **Baseline â‰  Inferior**: Para 60-70% dos casos, Ã© suficiente
2. **Simplicidade tem valor**: Menos pontos de falha, mais fÃ¡cil debug
3. **Foundation para otimizaÃ§Ã£o**: ImpossÃ­vel melhorar sem baseline para comparar
4. **Trade-offs claros**: Velocidade/custo vs precisÃ£o/completude

---

## ğŸ“ˆ Roadmap de Melhorias

```
Baseline RAG (vocÃª estÃ¡ aqui)
    â†“
    â”œâ”€â†’ HyDE (melhora ambiguidade)
    â”œâ”€â†’ Reranking (melhora precision)
    â”œâ”€â†’ Sub-Query (melhora recall)
    â”œâ”€â†’ Fusion (combina mÃºltiplas estratÃ©gias)
    â””â”€â†’ Graph RAG (queries multi-hop)
```

---

**PrÃ³xima TÃ©cnica**: [HyDE - Hypothetical Document Embeddings](./HYDE.md)
