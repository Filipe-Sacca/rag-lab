# Parent Document Retrieval - PrecisÃ£o na Busca, Contexto na Resposta

## ğŸ“‹ DefiniÃ§Ã£o

**Parent Document Retrieval** resolve o **dilema do tamanho de chunk**: buscar com chunks pequenos (alta precisÃ£o) mas retornar documentos completos (contexto rico).

A tÃ©cnica mantÃ©m **dois Ã­ndices**:
1. **Ãndice de busca**: Mini-chunks (128-256 tokens) - embeddings precisos
2. **Ãndice de contexto**: Documentos pais completos - informaÃ§Ã£o completa

**Insight**: Tamanho Ã³timo para busca â‰  Tamanho Ã³timo para geraÃ§Ã£o.

---

## ğŸ”„ Como Funciona

### Pipeline Completo

```
1. INDEXAÃ‡ÃƒO (Setup - Dois NÃ­veis)
   â”œâ”€ Documento original: "financial_report_q3.pdf"
   â”‚
   â”œâ”€ Criar mini-chunks (128 tokens):
   â”‚  â”œâ”€ Child 1: "Lucro Q3 foi R$ 3bi..."
   â”‚  â”œâ”€ Child 2: "Margem operacional 15%..."
   â”‚  â”œâ”€ Child 3: "Investimento P&D R$ 500mi..."
   â”‚  â””â”€ [Total: 20 mini-chunks]
   â”‚
   â”œâ”€ Armazenar mini-chunks no Vector DB
   â”‚  â””â”€ Com metadata: {parent_id: "doc_123"}
   â”‚
   â””â”€ Armazenar documento pai separadamente
      â””â”€ Document Store: {id: "doc_123", content: [...]}

2. RETRIEVAL (Runtime)
   â”œâ”€ Query: "Qual o lucro Q3?"
   â”œâ”€ Buscar nos MINI-CHUNKS (precisÃ£o)
   â”‚  â””â”€ Match: Child 1 (score: 0.95)
   â””â”€ Retornar DOCUMENTO PAI completo
      â””â”€ Lookup: parent_id â†’ doc_123 (contexto)

3. GERAÃ‡ÃƒO
   â”œâ”€ Prompt com documento PAI (nÃ£o mini-chunk)
   â”œâ”€ LLM tem contexto completo
   â””â”€ Resposta rica e detalhada
```

### ComparaÃ§Ã£o Visual

**Baseline RAG (chunks mÃ©dios 512 tokens):**
```
Documento â†’ Chunks 512 tokens â†’ Embed â†’ Busca â†’ Retorna chunks 512
                â†“
     PrecisÃ£o OK, Contexto OK (compromise)
```

**Parent Document:**
```
Documento â†’ Split em duas camadas:
  â”œâ”€ Mini-chunks 128 tokens â†’ Embed â†’ Busca (PRECISÃƒO)
  â””â”€ Documento completo â†’ Store â†’ Retorna (CONTEXTO)
                                    â†“
                          Melhor dos dois mundos
```

---

## ğŸ’¡ Por Que Funciona?

### Problema: Chunk Size Dilemma

**Chunks Pequenos (128 tokens):**
```
âœ… Embedding mais preciso (conceito Ãºnico)
âœ… Match de similaridade mais exato
âœ… Menos ruÃ­do
âŒ Contexto insuficiente para LLM
âŒ Pode perder informaÃ§Ã£o adjacente
```

**Chunks Grandes (1024 tokens):**
```
âœ… Contexto rico
âœ… InformaÃ§Ã£o completa
âŒ Embedding genÃ©rico (mÃºltiplos conceitos)
âŒ Match de similaridade impreciso
âŒ Muito ruÃ­do
```

**Parent Document = Best of Both:**
```
Busca com 128 tokens (precisÃ£o)
  â†“
Encontra chunk exato: "Lucro Q3: R$ 3bi"
  â†“
Retorna documento completo que CONTÃ‰M esse chunk
  â†“
LLM vÃª: Contexto de todo relatÃ³rio Q3
```

---

## ğŸ”¬ Exemplo PrÃ¡tico Detalhado

### Caso 1: Query EspecÃ­fica com Necessidade de Contexto

**Documento Original** (2000 tokens):
```
RELATÃ“RIO FINANCEIRO Q3 2024

Resumo Executivo:
A empresa teve desempenho excepcional no terceiro trimestre...

Resultados Financeiros:
Lucro lÃ­quido: R$ 3 bilhÃµes
Crescimento YoY: 15%
Margem operacional: 18%

AnÃ¡lise por Segmento:
- Cloud: R$ 1.5bi (50% do lucro)
- Hardware: R$ 1.0bi (33%)
- ServiÃ§os: R$ 500mi (17%)

Investimentos:
P&D: R$ 800mi (+20% vs Q2)
Marketing: R$ 200mi

ProjeÃ§Ãµes Q4:
Esperamos manter crescimento...
```

**IndexaÃ§Ã£o:**
```python
# Mini-chunks (128 tokens cada):
chunk_1 = "Lucro lÃ­quido: R$ 3 bilhÃµes. Crescimento YoY: 15%"
chunk_2 = "Margem operacional: 18%"
chunk_3 = "Cloud: R$ 1.5bi (50% do lucro)"
chunk_4 = "P&D: R$ 800mi (+20% vs Q2)"
# ... etc

# Cada chunk tem metadata:
metadata = {
    "parent_id": "financial_q3_2024",
    "chunk_index": 1,
    "source": "financial_report_q3.pdf"
}
```

**Query:**
```
"Qual foi o lucro do Q3 e como ele se distribui por segmento?"
```

**Baseline RAG (chunk 512 tokens):**
```python
# Busca retorna chunk mÃ©dio:
chunk_retrieved = """
Resultados Financeiros:
Lucro lÃ­quido: R$ 3 bilhÃµes
Crescimento YoY: 15%
Margem operacional: 18%
... [mais 300 tokens de contexto genÃ©rico]
"""

# âŒ Chunk TEM o lucro, mas NÃƒO tem distribuiÃ§Ã£o por segmento
# âŒ LLM responde: "Lucro R$ 3bi, mas nÃ£o tenho info sobre segmentos"
```

**Parent Document:**
```python
# Busca encontra mini-chunk preciso:
mini_chunk = "Lucro lÃ­quido: R$ 3 bilhÃµes. Crescimento YoY: 15%"

# Retorna documento PAI completo:
parent_doc = """
[RELATÃ“RIO COMPLETO 2000 tokens]
Inclui: Lucro + Margem + Segmentos + Investimentos + ProjeÃ§Ãµes
"""

# âœ… LLM responde:
# "Lucro Q3: R$ 3bi. DistribuiÃ§Ã£o: Cloud R$ 1.5bi (50%),
#  Hardware R$ 1.0bi (33%), ServiÃ§os R$ 500mi (17%)"
```

**Resultado**: Recall completo com precisÃ£o na busca.

---

### Caso 2: Evitar FragmentaÃ§Ã£o de InformaÃ§Ã£o

**Documento: Tutorial Machine Learning** (1500 tokens)

**Baseline (chunks 512):**
```python
# Documento fragmentado em 3 chunks:
chunk_1 = "IntroduÃ§Ã£o ML... tipos de algoritmos..."
chunk_2 = "Exemplo prÃ¡tico: modelo de regressÃ£o..."
chunk_3 = "CÃ³digo Python para treinar modelo..."

# Query: "Como treinar modelo de regressÃ£o?"

# Busca retorna chunk_2 (exemplo)
# âŒ Falta chunk_3 (cÃ³digo)
# âŒ Resposta incompleta
```

**Parent Document:**
```python
# Mini-chunks (128 tokens):
mini_1 = "Modelo de regressÃ£o linear prediz valores..."
mini_2 = "Exemplo: prever preÃ§o casa baseado em features..."
mini_3 = "CÃ³digo Python: from sklearn..."

# Busca encontra mini_2 (match exato "modelo regressÃ£o")

# Retorna documento PAI completo:
# âœ… Inclui: IntroduÃ§Ã£o + Exemplo + CÃ³digo completo
# âœ… Resposta completa com tudo conectado
```

---

## âš™ï¸ ConfiguraÃ§Ã£o PadrÃ£o

| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Child Chunk Size** | 128-256 tokens | Balance precisÃ£o/granularidade |
| **Parent Type** | Documento completo OU seÃ§Ã£o | Depende do domÃ­nio |
| **Top-K Children** | 3-5 | MÃºltiplos matches = mais pais |
| **Deduplication** | Sim | Evitar retornar mesmo pai 2x |
| **Max Parent Tokens** | 2048-4096 | Limite do context window |

### EstratÃ©gias de Parentesco

| EstratÃ©gia | Child | Parent | Uso |
|------------|-------|--------|-----|
| **Document-Level** | Mini-chunk 128 | Doc completo | Docs pequenos (<2K tokens) |
| **Section-Level** | ParÃ¡grafo | SeÃ§Ã£o completa | Docs grandes (>10K tokens) |
| **Sliding Window** | SentenÃ§a | Janela Â±5 sentenÃ§as | MÃ¡xima precisÃ£o |

---

## âœ… Vantagens

### 1. Resolve Chunk Size Dilemma
```
NÃ£o precisa mais escolher entre:
- Chunks pequenos (precisÃ£o) OU
- Chunks grandes (contexto)

Tem AMBOS simultaneamente
```

### 2. Precision Massiva
```
Mini-chunk embedding:
"Lucro Q3: R$ 3bi" â†’ Vector focado

vs Baseline (chunk 512 tokens):
"Resumo executivo... lucro... margem... investimentos..."
â†’ Vector genÃ©rico

Precision: 0.70 â†’ 0.90 (+28%)
```

### 3. Contexto Completo
```
LLM recebe documento inteiro:
â†’ Pode sintetizar informaÃ§Ãµes de mÃºltiplas partes
â†’ Resposta mais rica e conectada
â†’ Reduz "NÃ£o tenho essa informaÃ§Ã£o" (recall +30%)
```

### 4. Simples de Implementar
```python
# Apenas 2 passos extras:
1. Split em mini-chunks + armazenar parent_id
2. Lookup de parent apÃ³s retrieval

Complexidade: Baixa (vs Graph RAG)
```

### 5. Funciona com Qualquer TÃ©cnica
```
Parent Document + HyDE = Ã³timo
Parent Document + Reranking = Ã³timo
Parent Document + Sub-Query = Ã³timo

Ã‰ uma "camada" complementar
```

---

## âŒ Desvantagens

### 1. Contexto Pode Ser Excessivo
```
Query: "Qual o telefone?"

Mini-chunk match: "Telefone: (11) 1234-5678"

Parent retornado: Documento 2000 tokens sobre empresa
â†’ Inclui: HistÃ³ria, missÃ£o, valores, contatos...

âŒ 1900 tokens irrelevantes para LLM
âŒ Custo desnecessÃ¡rio
```

### 2. DeduplicaÃ§Ã£o NecessÃ¡ria
```
Query: "Lucro e margem Q3"

Matches:
- Mini-chunk A: "Lucro R$ 3bi" â†’ Parent: doc_123
- Mini-chunk B: "Margem 18%" â†’ Parent: doc_123

âŒ Sem dedup: Retorna doc_123 duas vezes
â†’ DesperdiÃ§a tokens
```

### 3. Limite de Context Window
```
Top-5 mini-chunks de 5 documentos diferentes:
â†’ 5 parents Ã— 2000 tokens = 10K tokens

Se LLM tem limite 8K:
âŒ NÃ£o cabe tudo
SoluÃ§Ã£o: Comprimir ou limitar top-k
```

### 4. Parent Pode Ser Muito Grande
```
Parent = PDF de 50 pÃ¡ginas (50K tokens)

âŒ ImpossÃ­vel passar para LLM
SoluÃ§Ã£o: Usar Section-Level (nÃ£o Document-Level)
```

### 5. IndexaÃ§Ã£o Mais Lenta
```
1 documento â†’ 20 mini-chunks
vs Baseline: 1 documento â†’ 4 chunks

IndexaÃ§Ã£o: 5x mais chunks para embedar
Tempo: +80-100%
```

### 6. Custo de Storage Aumenta
```
Baseline:
- Vector DB: 1000 chunks

Parent Document:
- Vector DB: 5000 mini-chunks
- Document Store: 1000 parents

Storage: +400-500%
```

---

## ğŸ“Š MÃ©tricas Esperadas

### RAGAS Scores vs Baseline

| MÃ©trica | Baseline (512 tokens) | Parent Document | Î” |
|---------|----------------------|-----------------|---|
| **Faithfulness** | 0.75-0.85 | 0.85-0.92 | +10-15% |
| **Answer Relevancy** | 0.70-0.85 | 0.82-0.93 | +15-20% |
| **Context Precision** | 0.60-0.75 | 0.80-0.92 | +25-35% â­ |
| **Context Recall** | 0.50-0.70 | 0.75-0.90 | +35-50% â­ |

### Performance

| MÃ©trica | Baseline | Parent Document |
|---------|----------|-----------------|
| **LatÃªncia** | 1.2-2.5s | 1.5-3.0s |
| **Custo/Query** | $0.001-0.003 | $0.002-0.005 |
| **IndexaÃ§Ã£o Time** | 30 min | 50-60 min |
| **Storage** | 1x | 4-5x |

---

## ğŸ¯ Quando Usar Parent Document

### âœ… Casos Ideais

**1. Documentos Estruturados com SeÃ§Ãµes**
```
âœ… RelatÃ³rios (financeiros, tÃ©cnicos)
âœ… Artigos acadÃªmicos (abstract, intro, methods...)
âœ… DocumentaÃ§Ã£o tÃ©cnica (seÃ§Ãµes lÃ³gicas)
```

**2. InformaÃ§Ã£o Interconectada**
```
âœ… Tutorial completo (passos dependem uns dos outros)
âœ… AnÃ¡lises (contexto geral + detalhes especÃ­ficos)
âœ… Casos de uso (setup + implementaÃ§Ã£o + resultados)
```

**3. Queries EspecÃ­ficas com Necessidade de Contexto**
```
âœ… "Qual a conclusÃ£o do experimento X?"
   â†’ Busca "experimento X", retorna seÃ§Ã£o completa com mÃ©todos + resultados + conclusÃ£o
```

**4. Documentos Pequenos-MÃ©dios (<5K tokens)**
```
âœ… Parent = documento completo cabe no context window
âœ… NÃ£o precisa comprimir ou truncar
```

**5. Alta PrecisÃ£o NecessÃ¡ria**
```
âœ… Legal, compliance (precisa encontrar clÃ¡usula EXATA)
âœ… Medicina (sintoma especÃ­fico â†’ contexto completo)
```

---

### âŒ Quando NÃƒO Usar

**1. Documentos Muito Grandes**
```
âŒ PDFs de 100+ pÃ¡ginas
âŒ Parent = 50K tokens (nÃ£o cabe no LLM)
â†’ Use Section-Level ou Baseline com chunks mÃ©dios
```

**2. InformaÃ§Ã£o Fragmentada e Independente**
```
âŒ FAQs (cada pergunta independente)
âŒ GlossÃ¡rios (definiÃ§Ãµes isoladas)
â†’ Baseline suficiente
```

**3. Queries que NÃƒO Precisam de Contexto**
```
âŒ "Qual o telefone?" â†’ SÃ³ precisa do nÃºmero
âŒ "PreÃ§o do produto X?" â†’ SÃ³ precisa do valor
â†’ Parent adiciona ruÃ­do desnecessÃ¡rio
```

**4. Budget de Storage Limitado**
```
âŒ 5x mais chunks para armazenar
âŒ Custo Pinecone aumenta proporcionalmente
```

**5. LatÃªncia CrÃ­tica**
```
âŒ Retornar parent grande = mais tokens para LLM
âŒ +30-50% latÃªncia vs baseline
```

---

## ğŸ”¬ Experimentos Recomendados

### 1. Child Chunk Size Optimization
```python
# Testar: 64, 128, 256, 512 tokens
# Medir: Precision vs Recall
# HipÃ³tese: 128-256 = sweet spot
```

### 2. Parent Granularity
```python
# Testar:
# - Document-level (doc completo)
# - Section-level (por seÃ§Ã£o)
# - Paragraph-level (parÃ¡grafo)
# Medir: Context relevance vs tokens usados
```

### 3. Deduplication Strategy
```python
# Quando mÃºltiplos children â†’ mesmo parent:
# - Retornar parent 1x (dedup)
# - Retornar parent mÃºltiplas vezes (reforÃ§o)
# - Merge de mÃºltiplos parents
# Medir: Token efficiency vs recall
```

---

## ğŸ’» Estrutura de CÃ³digo

```python
# parent_document.py

from typing import List, Dict

class ParentDocumentRAG:
    """
    Parent Document Retrieval: Busca mini-chunks, retorna parents.

    Pipeline:
    1. Retrieval com mini-chunks (precisÃ£o)
    2. Lookup de parents (contexto)
    3. DeduplicaÃ§Ã£o
    4. LLM generation
    """

    def __init__(self, pinecone_index, embeddings, llm, document_store):
        self.index = pinecone_index  # Mini-chunks
        self.embeddings = embeddings
        self.llm = llm
        self.doc_store = document_store  # Parent documents

        self.child_chunk_size = 128
        self.k_children = 10  # Buscar 10 mini-chunks

    def index_document(self, document: str, doc_id: str):
        """
        Indexa documento em dois nÃ­veis.
        """
        # 1. Criar mini-chunks
        mini_chunks = self._split_into_mini_chunks(document, self.child_chunk_size)

        # 2. Armazenar parent no document store
        self.doc_store.add({
            "id": doc_id,
            "content": document
        })

        # 3. Embed e armazenar mini-chunks no Vector DB
        for i, chunk in enumerate(mini_chunks):
            chunk_id = f"{doc_id}_child_{i}"
            vector = self.embeddings.embed_query(chunk)

            self.index.upsert([(
                chunk_id,
                vector,
                {
                    "text": chunk,
                    "parent_id": doc_id,  # â­ Link para parent
                    "chunk_index": i
                }
            )])

    def retrieve_children(self, query: str) -> List[Dict]:
        """
        Busca mini-chunks (children).
        """
        query_vector = self.embeddings.embed_query(query)

        results = self.index.query(
            vector=query_vector,
            top_k=self.k_children,
            include_metadata=True
        )

        children = []
        for match in results['matches']:
            children.append({
                "text": match['metadata']['text'],
                "parent_id": match['metadata']['parent_id'],
                "score": match['score']
            })

        return children

    def retrieve_parents(self, children: List[Dict]) -> List[str]:
        """
        Recupera documentos parents e deduplica.
        """
        # Coletar parent_ids Ãºnicos
        parent_ids = list(set([c['parent_id'] for c in children]))

        # Buscar parents no document store
        parents = []
        for parent_id in parent_ids:
            parent_doc = self.doc_store.get(parent_id)
            if parent_doc:
                parents.append(parent_doc['content'])

        return parents

    def generate(self, query: str, parents: List[str]) -> str:
        """
        GeraÃ§Ã£o com documentos parents completos.
        """
        # Montar contexto com parents
        context = "\n\n---\n\n".join(parents)

        prompt = f"""
Contexto (documentos completos):
{context}

Pergunta: {query}

Responda baseado no contexto acima.
"""

        response = self.llm.invoke(prompt, temperature=0.0)
        return response.content

    def query(self, query: str) -> Dict:
        """
        Pipeline completo Parent Document RAG.
        """
        start_time = time.time()

        # 1. Retrieve mini-chunks
        t1 = time.time()
        children = self.retrieve_children(query)
        children_time = time.time() - t1

        # 2. Retrieve parents
        t2 = time.time()
        parents = self.retrieve_parents(children)
        parents_time = time.time() - t2

        # 3. Generation
        t3 = time.time()
        response = self.generate(query, parents)
        generation_time = time.time() - t3

        total_latency = time.time() - start_time

        # Calcular tokens
        total_tokens = sum(len(p.split()) for p in parents)

        return {
            "response": response,
            "children_matched": children,
            "parents_retrieved": len(parents),
            "metrics": {
                "latency_total": total_latency,
                "latency_children": children_time,
                "latency_parents": parents_time,
                "latency_generation": generation_time,
                "children_count": len(children),
                "parents_count": len(parents),
                "context_tokens": total_tokens,
                "technique": "parent_document"
            }
        }

    def _split_into_mini_chunks(self, text: str, chunk_size: int) -> List[str]:
        """
        Split texto em mini-chunks de tamanho fixo.
        """
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i+chunk_size])
            chunks.append(chunk)

        return chunks
```

---

## ğŸ“ VariaÃ§Ãµes AvanÃ§adas

### 1. Section-Level Parents
```python
def index_with_sections(document, sections):
    """
    Parent = seÃ§Ã£o (nÃ£o documento completo).
    """
    for section in sections:
        # Mini-chunks dentro da seÃ§Ã£o
        mini_chunks = split(section.content, 128)

        # Parent = seÃ§Ã£o
        parent_id = f"{doc_id}_{section.title}"
        doc_store.add(parent_id, section.content)

        # Children linkam para seÃ§Ã£o
        for chunk in mini_chunks:
            index.add(chunk, parent_id=parent_id)
```

### 2. Sliding Window Parents
```python
def sliding_window_parents(text):
    """
    Child = sentenÃ§a
    Parent = Â±5 sentenÃ§as ao redor
    """
    sentences = split_sentences(text)

    for i, sentence in enumerate(sentences):
        # Child = sentenÃ§a Ãºnica
        child = sentence

        # Parent = janela ao redor
        start = max(0, i - 5)
        end = min(len(sentences), i + 6)
        parent = " ".join(sentences[start:end])

        index.add(child, parent_content=parent)
```

---

## ğŸ“š ReferÃªncias

**Papers:**
- LangChain Documentation - "Parent Document Retriever"
- Pinecone - "Advanced RAG: Parent-Child Chunking"

**ImplementaÃ§Ãµes:**
- LangChain: `ParentDocumentRetriever`
- LlamaIndex: `DocumentSummaryIndex` (similar)

---

## ğŸ¯ Aprendizados Chave

1. **Chunk Size Dilemma Resolvido**: PrecisÃ£o (busca) + Contexto (geraÃ§Ã£o)
2. **Simples mas Poderoso**: +30% recall com implementaÃ§Ã£o fÃ¡cil
3. **Complementar**: Funciona com HyDE, Reranking, Sub-Query
4. **Trade-off Storage**: 5x mais chunks, mas vale a pena
5. **Production-Ready**: Usado amplamente em sistemas reais

---

**TÃ©cnica Anterior**: [Graph RAG](./GRAPH_RAG.md)
**PrÃ³xima TÃ©cnica**: [Agentic RAG](./AGENTIC_RAG.md)
