# Reranking - Refinamento de Resultados com Cross-Encoder

## ğŸ“‹ DefiniÃ§Ã£o

**Reranking** Ã© uma tÃ©cnica de **pÃ³s-processamento** que reordena os chunks recuperados usando um modelo mais sofisticado de relevÃ¢ncia.

Funciona em **duas fases**:
1. **Retrieval rÃ¡pido** (bi-encoder): Busca inicial com muitos candidatos (top-50)
2. **Reranking preciso** (cross-encoder): Reordena para selecionar os melhores (top-5)

**Insight**: Bi-encoders sÃ£o rÃ¡pidos mas imprecisos. Cross-encoders sÃ£o lentos mas muito precisos.

---

## ğŸ”„ Como Funciona

### Pipeline Completo

```
1. RETRIEVAL INICIAL (Fase RÃ¡pida)
   â”œâ”€ Query: "Qual o lucro Q3?"
   â”œâ”€ Embedding com bi-encoder (text-embedding-004)
   â”œâ”€ Busca de similaridade no Pinecone
   â””â”€ Retornar top-k_initial (k=50) chunks

2. RERANKING (Fase Precisa) â­ NOVO
   â”œâ”€ Para cada chunk dos 50:
   â”‚  â”œâ”€ Concatenar [Query + Chunk]
   â”‚  â”œâ”€ Cross-encoder calcula score de relevÃ¢ncia
   â”‚  â””â”€ Score: 0.0 (irrelevante) - 1.0 (perfeito)
   â”œâ”€ Reordenar chunks por score
   â””â”€ Selecionar top-k_final (k=5) melhores

3. GERAÃ‡ÃƒO
   â”œâ”€ Montar prompt com top-5 reranqueados
   â”œâ”€ LLM gera resposta
   â””â”€ Retornar resposta final
```

### ComparaÃ§Ã£o Visual

**Baseline RAG:**
```
Query â†’ Embed â†’ Busca top-5 â†’ LLM
                     â†“
               [Pode ter ruÃ­do]
```

**Reranking:**
```
Query â†’ Embed â†’ Busca top-50 â†’ Rerank â†’ Top-5 limpo â†’ LLM
                                 â†“
                         [Filtra ruÃ­do]
```

---

## ğŸ§  Bi-Encoder vs Cross-Encoder

### Bi-Encoder (Retrieval)

**Como funciona:**
```python
# Processa separadamente
query_vector = encode(query)        # [0.12, -0.45, ...]
doc_vector = encode(document)       # [0.18, -0.32, ...]

# Compara vetores
similarity = cosine(query_vector, doc_vector)
```

**CaracterÃ­sticas:**
- âœ… **RÃ¡pido**: Documentos jÃ¡ estÃ£o prÃ©-embedados
- âœ… **EscalÃ¡vel**: MilhÃµes de docs em <100ms
- âŒ **Impreciso**: NÃ£o vÃª query + doc juntos

---

### Cross-Encoder (Reranking)

**Como funciona:**
```python
# Processa JUNTOS
input = "[CLS] query [SEP] document [SEP]"
relevance_score = cross_encoder(input)  # 0.0 - 1.0
```

**CaracterÃ­sticas:**
- âœ… **Muito Preciso**: VÃª interaÃ§Ã£o query â†” doc
- âœ… **Captura Nuances**: Entende contexto completo
- âŒ **Lento**: Precisa processar cada par
- âŒ **NÃ£o EscalÃ¡vel**: ImpossÃ­vel para milhÃµes de docs

---

## ğŸ’¡ Por Que Funciona?

### Problema do Bi-Encoder

```python
Query: "Qual empresa teve maior lucro?"

# Bi-encoder (embeddings independentes):
Doc A: "Amazon teve receita de $500B"
  â†’ query_vec Â· doc_a_vec = 0.82 (alta similaridade por "empresa", "$")

Doc B: "O lucro da Apple foi $100B, maior que Microsoft $80B"
  â†’ query_vec Â· doc_b_vec = 0.79 (menor, mas Ã‰ A RESPOSTA!)

# âŒ Ranqueamento errado: A antes de B
```

### SoluÃ§Ã£o Cross-Encoder

```python
# Cross-encoder vÃª query + doc juntos:

Input A: "Qual empresa teve maior lucro? [SEP] Amazon teve receita $500B"
  â†’ Score: 0.45 (receita â‰  lucro)

Input B: "Qual empresa teve maior lucro? [SEP] Lucro Apple $100B > Microsoft $80B"
  â†’ Score: 0.95 (responde EXATAMENTE a pergunta)

# âœ… Ranqueamento correto: B antes de A
```

---

## ğŸ”¬ Exemplo PrÃ¡tico Detalhado

### Caso 1: Eliminar RuÃ­do

**Query:**
```
"PolÃ­tica de trabalho remoto da empresa"
```

**Retrieval Inicial (top-50):**
```python
# Bi-encoder retorna (misturado):
1. "PolÃ­tica remoto: 3 dias/semana..."           Score: 0.89 âœ…
2. "Remote desktop para acesso VPN..."           Score: 0.87 âŒ (tech, nÃ£o RH)
3. "Trabalho em equipe remota distribuÃ­da..."    Score: 0.85 âŒ (conceito, nÃ£o polÃ­tica)
4. "BenefÃ­cios: plano saÃºde, remoto..."          Score: 0.84 âœ…
5. "PolÃ­tica de fÃ©rias e afastamentos..."        Score: 0.82 âŒ
...
48. "Trabalho remoto permitido com aprovaÃ§Ã£o..." Score: 0.61 âœ… (RELEVANTE mas baixo!)
```

**Reranking (Cross-Encoder):**
```python
# Avalia query + cada chunk:
1. "PolÃ­tica remoto: 3 dias/semana..."           Score: 0.96 âœ…
48. "Trabalho remoto permitido com aprovaÃ§Ã£o..." Score: 0.91 âœ… (SUBIU!)
4. "BenefÃ­cios: plano saÃºde, remoto..."          Score: 0.78 âœ…
2. "Remote desktop para acesso VPN..."           Score: 0.12 âŒ (FILTRADO!)
3. "Trabalho em equipe remota distribuÃ­da..."    Score: 0.34 âŒ (FILTRADO!)

# Top-5 final: TODOS relevantes
```

**Resultado**: Context Precision de 0.60 â†’ 0.95

---

### Caso 2: Capturar NegaÃ§Ã£o

**Query:**
```
"Produtos que NÃƒO contÃªm glÃºten"
```

**Bi-Encoder (Falha):**
```python
# Embeddings nÃ£o capturam "NÃƒO"
Doc A: "Produto X contÃ©m glÃºten"          Score: 0.88 âŒ
Doc B: "Produto Y Ã© livre de glÃºten"      Score: 0.85 âœ…

# âŒ Ranqueia doc ERRADO em 1Âº
```

**Cross-Encoder (Sucesso):**
```python
Input A: "NÃƒO contÃªm glÃºten [SEP] Produto X contÃ©m glÃºten"
  â†’ Score: 0.05 âŒ (detecta contradiÃ§Ã£o!)

Input B: "NÃƒO contÃªm glÃºten [SEP] Produto Y livre de glÃºten"
  â†’ Score: 0.98 âœ… (match perfeito)

# âœ… Ranqueia corretamente
```

---

## âš™ï¸ ConfiguraÃ§Ã£o PadrÃ£o

| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Initial Top-K** | 50 | Suficiente para recall, nÃ£o sobrecarrega reranker |
| **Final Top-K** | 5 | Mesma quantidade do baseline para LLM |
| **Reranker Model** | ms-marco-MiniLM-L6 | Balance velocidade/precisÃ£o |
| **Batch Size** | 32 | Processa chunks em paralelo |
| **Score Threshold** | 0.3 | Descarta chunks muito irrelevantes |

### Modelos de Reranking

| Modelo | ParÃ¢metros | LatÃªncia | PrecisÃ£o | Uso |
|--------|------------|----------|----------|-----|
| **MiniLM-L6** | 22M | ~50ms | â­â­â­ | ProduÃ§Ã£o (recomendado) |
| **BERT-base** | 110M | ~200ms | â­â­â­â­ | Alta precisÃ£o |
| **Cohere Rerank** | API | ~100ms | â­â­â­â­â­ | Melhor (pago) |

---

## âœ… Vantagens

### 1. PrecisÃ£o Massiva
```
Context Precision: 0.60 â†’ 0.90-0.95 (+50%)
Elimina ~80% do ruÃ­do do retrieval inicial
```

### 2. Recall Melhorado
```
Buscar top-50 inicial captura chunks "escondidos"
Reranking traz os melhores para top-5
Recall: 0.65 â†’ 0.85 (+30%)
```

### 3. Captura Nuances SemÃ¢nticas
```
âœ… NegaÃ§Ãµes: "nÃ£o contÃ©m"
âœ… ComparaÃ§Ãµes: "maior que", "melhor"
âœ… Condicionais: "se... entÃ£o"
âœ… Contexto: Entende relaÃ§Ã£o query â†” doc
```

### 4. Independente da Query
```
Funciona para:
- Queries simples
- Queries complexas
- Qualquer domÃ­nio
â†’ Melhoria consistente
```

### 5. Combina com Outras TÃ©cnicas
```
HyDE + Reranking: PrecisÃ£o mÃ¡xima
Sub-Query + Reranking: Recall + Precision
Fusion + Reranking: Melhor de todos mundos
```

---

## âŒ Desvantagens

### 1. LatÃªncia Adicional
```
Baseline: 1.2s
Reranking: 1.8-2.5s (+0.6-1.3s)

Breakdown:
- Retrieval top-50: 0.1s
- Reranking 50 chunks: +0.5-1.0s (gargalo!)
- LLM: 1.0s
```

### 2. Custo Computacional
```
# Bi-encoder (retrieval):
- 1 embedding da query
- 1 busca vetorial

# Cross-encoder (reranking):
- 50 inferÃªncias do modelo
- GPU recomendada (senÃ£o +2-3s)

Custo compute: +40-60%
```

### 3. DependÃªncia de Modelo Externo
```
OpÃ§Ãµes:
1. Self-hosted: Precisa GPU/CPU potente
2. API (Cohere): $1/1K requests (caro em escala)

Complexidade operacional aumenta
```

### 4. Overhead para Queries Simples
```
Query: "Qual o telefone?"
Baseline: 5 chunks, 4 corretos â†’ Precision 0.8

âŒ Reranking: +0.8s para ganhar apenas 0.2 precision
â†’ NÃ£o vale o trade-off
```

### 5. Limite de Contexto do Reranker
```
Cross-encoder tÃ­pico: Max 512 tokens

Chunk grande (1024 tokens):
âŒ Precisa truncar â†’ Perde informaÃ§Ã£o
```

### 6. NÃ£o Resolve Retrieval Ruim
```
Se retrieval inicial (top-50) nÃ£o capturou chunk relevante:
âŒ Reranking nÃ£o cria chunk do nada

"Garbage in, garbage out"
```

---

## ğŸ“Š MÃ©tricas Esperadas

### RAGAS Scores vs Baseline

| MÃ©trica | Baseline | Reranking | Î” |
|---------|----------|-----------|---|
| **Faithfulness** | 0.75-0.85 | 0.85-0.92 | +10-15% |
| **Answer Relevancy** | 0.70-0.85 | 0.85-0.95 | +15-20% |
| **Context Precision** | 0.60-0.75 | 0.85-0.95 | +35-50% â­ |
| **Context Recall** | 0.50-0.70 | 0.70-0.90 | +30-40% â­ |

### Performance

| MÃ©trica | Baseline | Reranking |
|---------|----------|-----------|
| **LatÃªncia** | 1.2-2.5s | 1.8-3.5s |
| **Custo/Query** | $0.001-0.003 | $0.002-0.005 |
| **GPU Utilization** | 0% | 20-40% (self-hosted) |
| **Throughput** | ~40 q/min | ~25 q/min |

---

## ğŸ¯ Quando Usar Reranking

### âœ… Casos Ideais

**1. Retrieval com Muito RuÃ­do**
```
âœ… Vector DB grande (>100K docs)
âœ… Documentos similares entre si
âœ… Queries genÃ©ricas ("melhorar performance")
```

**2. Alta PrecisÃ£o CrÃ­tica**
```
âœ… Legal/Compliance (erro = risco)
âœ… MÃ©dico/SaÃºde (precisÃ£o = seguranÃ§a)
âœ… Financeiro/Auditoria
```

**3. Queries Complexas**
```
âœ… ComparaÃ§Ãµes: "Produto A vs B"
âœ… NegaÃ§Ãµes: "NÃ£o contÃ©m X"
âœ… Condicionais: "Se isso, entÃ£o..."
```

**4. Melhorar Recall + Precision**
```
âœ… Buscar top-50 (recall)
âœ… Reranquear para top-5 (precision)
â†’ Melhor dos dois mundos
```

**5. Combinar com Outras TÃ©cnicas**
```
âœ… HyDE (melhora retrieval) + Reranking (limpa ruÃ­do)
âœ… Sub-Query (aumenta recall) + Reranking (filtra)
```

---

### âŒ Quando NÃƒO Usar

**1. Queries Simples e Diretas**
```
âŒ "Qual o email?"
âŒ "PreÃ§o do produto X"
â†’ Baseline jÃ¡ tem precision alta
```

**2. Vector DB Pequeno e Limpo**
```
âŒ <1K documentos bem curados
âŒ Retrieval jÃ¡ retorna chunks Ã³timos
â†’ Overhead desnecessÃ¡rio
```

**3. LatÃªncia CrÃ­tica (<1s)**
```
âŒ Chatbot em tempo real
âŒ Autocompletar em busca
â†’ +0.8s degrada UX
```

**4. Budget Computacional Limitado**
```
âŒ Sem GPU disponÃ­vel
âŒ CPU lenta (reranking leva 2-3s)
â†’ InviÃ¡vel
```

**5. Alto Volume de Queries**
```
âŒ >100K queries/dia
âŒ Custo de reranking API = $100+/dia
â†’ NÃ£o escala financeiramente
```

---

## ğŸ”¬ Experimentos Recomendados

### 1. Initial Top-K Optimization
```python
# Testar: k=20, k=50, k=100, k=200
# Medir: Recall vs LatÃªncia
# HipÃ³tese: k=50 Ã© sweet spot (95% recall, latÃªncia OK)
```

### 2. Reranker Model Comparison
```python
# Testar:
# - MiniLM-L6 (rÃ¡pido)
# - BERT-base (mÃ©dio)
# - Cohere API (melhor)
# Medir: Precision vs LatÃªncia vs Custo
```

### 3. Score Threshold Impact
```python
# ApÃ³s reranking, filtrar chunks com score < threshold
# Testar: 0.0, 0.3, 0.5, 0.7
# Medir: Precision (pode subir ao eliminar ruÃ­do extremo)
```

### 4. Batch Size Tuning
```python
# Reranker processa chunks em batches
# Testar: batch_size = 8, 16, 32, 64
# Medir: Throughput (GPU utilization)
```

---

## ğŸ’» Estrutura de CÃ³digo

```python
# reranking.py

from sentence_transformers import CrossEncoder

class RerankingRAG:
    """
    RAG com reranking usando cross-encoder.

    Pipeline:
    1. Retrieval inicial (top-k_initial)
    2. Cross-encoder reranking
    3. SeleÃ§Ã£o top-k_final
    4. LLM generation
    """

    def __init__(self, pinecone_index, embeddings, llm, reranker_model="cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.index = pinecone_index
        self.embeddings = embeddings
        self.llm = llm

        # Cross-encoder para reranking
        self.reranker = CrossEncoder(reranker_model)

        self.k_initial = 50  # Retrieval inicial
        self.k_final = 5     # ApÃ³s reranking

    def retrieve(self, query: str) -> List[Document]:
        """
        Retrieval inicial com muitos candidatos.
        """
        query_vector = self.embeddings.embed_query(query)

        # Buscar top-50
        results = self.index.query(
            vector=query_vector,
            top_k=self.k_initial,
            include_metadata=True
        )

        candidates = self._parse_results(results)
        return candidates

    def rerank(self, query: str, candidates: List[Document]) -> List[Document]:
        """
        Reranquear usando cross-encoder.
        """
        # Preparar pares [query, doc]
        pairs = [[query, doc.page_content] for doc in candidates]

        # Cross-encoder calcula scores
        scores = self.reranker.predict(pairs)

        # Adicionar scores aos documentos
        for doc, score in zip(candidates, scores):
            doc.metadata['rerank_score'] = float(score)

        # Reordenar por score
        reranked = sorted(
            candidates,
            key=lambda x: x.metadata['rerank_score'],
            reverse=True
        )

        # Retornar top-k final
        return reranked[:self.k_final]

    def generate(self, query: str, context: List[Document]) -> str:
        """
        GeraÃ§Ã£o com LLM.
        """
        prompt = self._build_prompt(query, context)
        response = self.llm.invoke(prompt, temperature=0.0)
        return response.content

    def query(self, query: str) -> Dict:
        """
        Pipeline completo com mÃ©tricas.
        """
        start_time = time.time()

        # 1. Retrieval inicial
        t1 = time.time()
        candidates = self.retrieve(query)
        retrieval_time = time.time() - t1

        # 2. Reranking
        t2 = time.time()
        chunks = self.rerank(query, candidates)
        rerank_time = time.time() - t2

        # 3. Generation
        t3 = time.time()
        response = self.generate(query, chunks)
        generation_time = time.time() - t3

        total_latency = time.time() - start_time

        return {
            "response": response,
            "chunks": chunks,
            "metrics": {
                "latency_total": total_latency,
                "latency_retrieval": retrieval_time,
                "latency_rerank": rerank_time,
                "latency_generation": generation_time,
                "chunks_initial": len(candidates),
                "chunks_final": len(chunks),
                "technique": "reranking",
                "avg_rerank_score": sum(c.metadata['rerank_score'] for c in chunks) / len(chunks)
            }
        }
```

---

## ğŸ“ VariaÃ§Ãµes AvanÃ§adas

### 1. Cohere Rerank API
```python
import cohere

co = cohere.Client(api_key="...")

def rerank_with_cohere(query, docs):
    """
    Reranking com API Cohere (melhor precisÃ£o).
    """
    results = co.rerank(
        query=query,
        documents=[d.page_content for d in docs],
        top_n=5,
        model="rerank-english-v2.0"
    )

    # Reordenar docs originais
    reranked = [docs[r.index] for r in results]
    return reranked
```

**Vantagens**: Melhor modelo, sem GPU local
**Desvantagens**: $1/1K requests

---

### 2. Hybrid Reranking
```python
def hybrid_rerank(query, docs):
    """
    Combina cross-encoder + BM25 lexical.
    """
    # Cross-encoder (semÃ¢ntico)
    semantic_scores = cross_encoder.predict([[query, d] for d in docs])

    # BM25 (lexical keyword matching)
    lexical_scores = bm25.get_scores(query, docs)

    # Combinar (weighted average)
    final_scores = 0.7 * semantic_scores + 0.3 * lexical_scores

    # Reordenar
    return sorted(zip(docs, final_scores), key=lambda x: x[1], reverse=True)
```

**BenefÃ­cio**: Captura keyword + semÃ¢ntica

---

### 3. Two-Stage Reranking
```python
def two_stage_rerank(query, docs):
    """
    1Âº estÃ¡gio: MiniLM rÃ¡pido (top-50 â†’ top-20)
    2Âº estÃ¡gio: BERT preciso (top-20 â†’ top-5)
    """
    # Stage 1: RÃ¡pido
    stage1 = minilm_reranker.predict(query, docs, k=20)

    # Stage 2: Preciso
    stage2 = bert_reranker.predict(query, stage1, k=5)

    return stage2
```

**BenefÃ­cio**: Balance latÃªncia/precisÃ£o

---

## ğŸ“š ReferÃªncias

**Papers:**
- Nogueira et al. (2019) - "Document Ranking with Cross-Encoders"
- Pradeep et al. (2021) - "Dense vs. Sparse Retrieval for Question Answering"

**Modelos:**
- Sentence-Transformers: `cross-encoder/ms-marco-*`
- Cohere Rerank: API comercial
- OpenAI Embedding: NÃ£o tem reranker nativo

**Benchmarks:**
- MS MARCO: +15% MRR@10 vs bi-encoder alone
- BEIR: +12% nDCG@10

---

## ğŸ¯ Aprendizados Chave

1. **Duas Fases Essenciais**: Retrieval rÃ¡pido + Reranking preciso
2. **Trade-off LatÃªncia/PrecisÃ£o**: +0.8s para +50% precision (geralmente vale)
3. **Cross-Encoder â‰  Bi-Encoder**: Modelos diferentes, propÃ³sitos diferentes
4. **Top-50 Initial**: Sweet spot entre recall e latÃªncia de reranking
5. **Combina com Tudo**: Funciona com HyDE, Sub-Query, Fusion

---

## ğŸ“ˆ ProgressÃ£o de Complexidade

```
Baseline RAG
    â†“
Reranking (vocÃª estÃ¡ aqui)
    â†“
    â”œâ”€â†’ Hybrid Reranking (semÃ¢ntico + lexical)
    â”œâ”€â†’ Two-Stage (fast â†’ precise)
    â””â”€â†’ HyDE + Reranking (combo poderoso)
```

---

**TÃ©cnica Anterior**: [HyDE](./HYDE.md)
**PrÃ³xima TÃ©cnica**: [Sub-Query Decomposition](./SUBQUERY.md)
