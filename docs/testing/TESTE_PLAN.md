# ğŸ§ª Plano de Testes RAG Lab - ComparaÃ§Ã£o Head-to-Head

**Objetivo**: Testar as **mesmas 10 perguntas** em todas as 8 tÃ©cnicas RAG para comparaÃ§Ã£o objetiva

**Total de Queries**: 80 (10 perguntas Ã— 8 tÃ©cnicas)

---

## ğŸ“‹ 10 Perguntas Universais

Cada pergunta serÃ¡ testada em **todas as 8 tÃ©cnicas** (Baseline, HyDE, Reranking, Agentic, Fusion, Sub-Query, Graph, Adaptive)

### 1ï¸âƒ£ Pergunta Direta/Factual
**"O que Ã© RAG (Retrieval Augmented Generation)?"**
- Tipo: Factual, definitÃ³ria
- Esperado: Baseline deve ser rÃ¡pido e preciso
- Desafio: Ver se tÃ©cnicas avanÃ§adas agregam valor

---

### 2ï¸âƒ£ Pergunta Conceitual
**"Por que RAG Ã© mais confiÃ¡vel que usar apenas LLMs puros sem contexto?"**
- Tipo: Conceitual, explicativa
- Esperado: HyDE deve gerar resposta mais profunda
- Desafio: Ver como cada tÃ©cnica lida com abstraÃ§Ã£o

---

### 3ï¸âƒ£ Pergunta de PrecisÃ£o/Detalhes
**"Quais sÃ£o os 3 componentes principais de um sistema RAG e suas funÃ§Ãµes especÃ­ficas?"**
- Tipo: Lista exata, precisÃ£o
- Esperado: Reranking deve encontrar chunks mais relevantes
- Desafio: Ver qual tÃ©cnica Ã© mais precisa em detalhes

---

### 4ï¸âƒ£ Pergunta Comparativa
**"Compare embeddings com keyword search: vantagens e desvantagens de cada abordagem"**
- Tipo: ComparaÃ§Ã£o, mÃºltiplos aspectos
- Esperado: Sub-Query pode decompor bem
- Desafio: Ver como cada tÃ©cnica estrutura comparaÃ§Ãµes

---

### 5ï¸âƒ£ Pergunta AmbÃ­gua/MÃºltiplas InterpretaÃ§Ãµes
**"Como melhorar a qualidade de um sistema RAG?"**
- Tipo: Aberta, mÃºltiplas respostas vÃ¡lidas
- Esperado: Fusion deve combinar perspectivas
- Desafio: Ver como cada tÃ©cnica lida com ambiguidade

---

### 6ï¸âƒ£ Pergunta Sobre Relacionamentos
**"Qual a relaÃ§Ã£o entre chunk size, embeddings e qualidade do retrieval?"**
- Tipo: Relacionamentos, conexÃµes
- Esperado: Graph RAG deve explorar entidades
- Desafio: Ver como cada tÃ©cnica conecta conceitos

---

### 7ï¸âƒ£ Pergunta TÃ©cnica/ImplementaÃ§Ã£o
**"Como funciona o processo de chunking em RAG e quais sÃ£o os parÃ¢metros importantes?"**
- Tipo: TÃ©cnica, processo
- Esperado: Baseline/Reranking devem encontrar doc tÃ©cnico
- Desafio: Ver qual tÃ©cnica Ã© melhor para docs tÃ©cnicos

---

### 8ï¸âƒ£ Pergunta Complexa/Composta
**"Explique o pipeline completo de RAG desde o upload do documento atÃ© a geraÃ§Ã£o da resposta, incluindo todos os componentes e suas interaÃ§Ãµes"**
- Tipo: Complexa, multi-parte
- Esperado: Sub-Query deve decompor bem
- Desafio: Ver como cada tÃ©cnica lida com complexidade

---

### 9ï¸âƒ£ Pergunta de Trade-offs
**"Quais sÃ£o os trade-offs entre latÃªncia, custo e qualidade em diferentes tÃ©cnicas RAG?"**
- Tipo: Trade-offs, balanceamento
- Esperado: Agentic pode iterar para melhor anÃ¡lise
- Desafio: Ver qual tÃ©cnica analisa trade-offs melhor

---

### ğŸ”Ÿ Pergunta de Casos de Uso
**"Quando usar baseline RAG vs HyDE vs reranking? DÃª exemplos de casos de uso para cada"**
- Tipo: AplicaÃ§Ã£o prÃ¡tica, cenÃ¡rios
- Esperado: Adaptive deve rotear inteligentemente
- Desafio: Ver qual tÃ©cnica dÃ¡ recomendaÃ§Ãµes mais Ãºteis

---

## ğŸ“Š Matriz de Testes

| Pergunta | Baseline | HyDE | Reranking | Agentic | Fusion | SubQuery | Graph | Adaptive |
|----------|----------|------|-----------|---------|--------|----------|-------|----------|
| Q1: O que Ã© RAG | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q2: Por que RAG > LLM | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q3: 3 componentes | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q4: Compare embeddings | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q5: Como melhorar RAG | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q6: RelaÃ§Ã£o chunk/embed | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q7: Processo chunking | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q8: Pipeline completo | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q9: Trade-offs | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |
| Q10: Quando usar | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ | â±ï¸ |

**Total**: 80 testes

---

## ğŸ”„ Processo de Teste (2 Abordagens)

### ğŸ…°ï¸ Abordagem 1: Por Pergunta (Recomendado)
**Testar cada pergunta em todas as tÃ©cnicas antes de passar para prÃ³xima**

```
Q1: "O que Ã© RAG?"
â”œâ”€ Baseline
â”œâ”€ HyDE
â”œâ”€ Reranking
â”œâ”€ Agentic
â”œâ”€ Fusion
â”œâ”€ SubQuery
â”œâ”€ Graph
â””â”€ Adaptive

Q2: "Por que RAG > LLM?"
â”œâ”€ Baseline
â”œâ”€ HyDE
...
```

**Vantagens**:
- âœ… FÃ¡cil comparar resultados imediatos
- âœ… Detecta diferenÃ§as lado a lado
- âœ… Melhor para anÃ¡lise qualitativa

---

### ğŸ…±ï¸ Abordagem 2: Por TÃ©cnica
**Testar todas as perguntas em uma tÃ©cnica antes de mudar**

```
Baseline:
â”œâ”€ Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10

HyDE:
â”œâ”€ Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10
...
```

**Vantagens**:
- âœ… Mais rÃ¡pido (menos troca de dropdown)
- âœ… Melhor para anÃ¡lise quantitativa
- âœ… FÃ¡cil manter ritmo de teste

---

## ğŸ“ˆ MÃ©tricas a Coletar

Para **cada teste** (80 no total), anote:

### MÃ©tricas AutomÃ¡ticas (do sistema)
- â±ï¸ **LatÃªncia Total** (ms)
- ğŸ”¢ **NÃºmero de Sources** recuperados
- ğŸ“Š **Scores de Similaridade** (min/avg/max)
- ğŸ”§ **TÃ©cnica Real Usada** (importante para Agentic/Adaptive)

### MÃ©tricas Manuais (sua avaliaÃ§Ã£o)
- âœ… **Qualidade da Resposta** (1-5):
  - 1: NÃ£o responde
  - 2: Resposta parcial/incorreta
  - 3: Responde corretamente mas superficial
  - 4: Resposta boa e completa
  - 5: Resposta excelente, profunda e precisa

- ğŸ¯ **RelevÃ¢ncia dos Sources** (1-5):
  - 1: Nenhum source relevante
  - 2: Poucos sources relevantes
  - 3: Metade dos sources sÃ£o relevantes
  - 4: Maioria dos sources sÃ£o relevantes
  - 5: Todos sources sÃ£o altamente relevantes

- ğŸ’¬ **ObservaÃ§Ãµes**: Qualquer comportamento interessante/inesperado

---

## ğŸ“‹ Template de AnotaÃ§Ã£o

```
=== TESTE #X ===
Pergunta: [NÃºmero da pergunta]
TÃ©cnica: [Nome da tÃ©cnica]
Timestamp: [HH:MM:SS]

MÃ©tricas AutomÃ¡ticas:
- LatÃªncia: _____ms
- Sources: _____
- Scores: min=___ avg=___ max=___
- TÃ©cnica Real: ________

AvaliaÃ§Ã£o Manual:
- Qualidade Resposta: [ ] 1  [ ] 2  [ ] 3  [ ] 4  [ ] 5
- RelevÃ¢ncia Sources: [ ] 1  [ ] 2  [ ] 3  [ ] 4  [ ] 5
- ObservaÃ§Ãµes: ____________________
```

---

## ğŸ¯ AnÃ¡lise PÃ³s-Teste

### SQL: EstatÃ­sticas por TÃ©cnica
```sql
-- VisÃ£o geral por tÃ©cnica
SELECT
    technique,
    COUNT(*) as total_queries,
    ROUND(AVG(JSON_EXTRACT(metrics, '$.latency_ms')), 2) as avg_latency_ms,
    ROUND(MIN(JSON_EXTRACT(metrics, '$.latency_ms')), 2) as min_latency_ms,
    ROUND(MAX(JSON_EXTRACT(metrics, '$.latency_ms')), 2) as max_latency_ms,
    ROUND(AVG(JSON_EXTRACT(metrics, '$.num_sources')), 2) as avg_sources
FROM rag_executions
GROUP BY technique
ORDER BY avg_latency_ms;
```

### SQL: EstatÃ­sticas por Pergunta
```sql
-- Ver como cada pergunta se comportou
SELECT
    query,
    COUNT(DISTINCT technique) as techniques_tested,
    ROUND(AVG(JSON_EXTRACT(metrics, '$.latency_ms')), 2) as avg_latency_ms,
    ROUND(AVG(JSON_EXTRACT(metrics, '$.num_sources')), 2) as avg_sources
FROM rag_executions
GROUP BY query
ORDER BY avg_latency_ms DESC;
```

### SQL: Head-to-Head de 2 TÃ©cnicas
```sql
-- Comparar Baseline vs HyDE nas mesmas queries
SELECT
    r1.query,
    ROUND(r1.latency_ms, 2) as baseline_latency,
    ROUND(r2.latency_ms, 2) as hyde_latency,
    ROUND(r2.latency_ms - r1.latency_ms, 2) as diff_ms,
    r1.num_sources as baseline_sources,
    r2.num_sources as hyde_sources
FROM
    (SELECT * FROM rag_executions WHERE technique = 'baseline') r1
JOIN
    (SELECT * FROM rag_executions WHERE technique = 'hyde') r2
ON r1.query = r2.query
ORDER BY diff_ms DESC;
```

---

## ğŸ“Š HipÃ³teses a Validar

### âš¡ Performance
- **H1**: Baseline < HyDE < Reranking < Fusion < SubQuery (latÃªncia)
- **H2**: Agentic tem latÃªncia variÃ¡vel (depende de iteraÃ§Ãµes)
- **H3**: Adaptive adiciona overhead de classificaÃ§Ã£o (~100-200ms)

### ğŸ¯ Qualidade
- **H4**: HyDE Ã© melhor em perguntas conceituais (Q2, Q5)
- **H5**: Reranking Ã© melhor em perguntas de precisÃ£o (Q3, Q7)
- **H6**: Fusion Ã© melhor em perguntas ambÃ­guas (Q5)
- **H7**: SubQuery Ã© melhor em perguntas complexas (Q8)
- **H8**: Graph Ã© melhor em perguntas de relacionamento (Q6)

### ğŸ¤– Comportamento de Agentes
- **H9**: Agentic roteia corretamente baseado no tipo de query
- **H10**: Adaptive classifica queries corretamente
- **H11**: Agentic itera quando primeiro resultado Ã© insatisfatÃ³rio

### ğŸ“ˆ Scores
- **H12**: Reranking tem scores mais altos que Baseline (cross-encoder > bi-encoder)
- **H13**: HyDE pode ter scores mais baixos mas respostas melhores
- **H14**: Fusion normaliza scores via RRF

---

## âœ… Checklist de PreparaÃ§Ã£o

- [x] Banco de dados limpo (`rag_lab.db`: 0 registros)
- [x] Observability limpo (`events.db`: 0 eventos)
- [ ] Backend rodando (http://localhost:8000)
- [ ] Frontend rodando (http://localhost:9091)
- [ ] Documentos indexados no Pinecone
- [ ] Template de anotaÃ§Ã£o impresso/aberto
- [ ] CronÃ´metro/timer disponÃ­vel
- [ ] Papel ou planilha para anotaÃ§Ãµes

---

## ğŸš€ InstruÃ§Ãµes de ExecuÃ§Ã£o

### Passo a Passo

1. **Escolha a abordagem** (Por Pergunta ou Por TÃ©cnica)

2. **Para cada teste**:
   - Selecione a tÃ©cnica no dropdown
   - Copie a pergunta exatamente como estÃ¡
   - Cole no frontend
   - â±ï¸ Inicie o cronÃ´metro (se quiser validar latÃªncia)
   - Clique em "Enviar"
   - Aguarde a resposta completa
   - Anote as mÃ©tricas automÃ¡ticas (latÃªncia, sources, scores)
   - Avalie manualmente (qualidade, relevÃ¢ncia)
   - Anote observaÃ§Ãµes interessantes

3. **Salve os dados**:
   - As mÃ©tricas automÃ¡ticas jÃ¡ estÃ£o no banco
   - Suas avaliaÃ§Ãµes manuais devem ser registradas Ã  parte

4. **Ao final dos 80 testes**:
   - Execute as queries SQL de anÃ¡lise
   - Compare resultados quantitativos vs qualitativos
   - Valide as hipÃ³teses
   - Identifique pontos fortes/fracos de cada tÃ©cnica

---

## ğŸ¯ Objetivos do Teste

1. âœ… **ComparaÃ§Ã£o Objetiva**: Mesmas perguntas = comparaÃ§Ã£o justa
2. âœ… **Identificar Pontos Fortes**: Quando cada tÃ©cnica brilha
3. âœ… **Detectar Fraquezas**: Quando cada tÃ©cnica falha
4. âœ… **Validar Roteamento**: Agentic e Adaptive escolhem certo?
5. âœ… **Trade-off Analysis**: LatÃªncia vs Qualidade vs Custo
6. âœ… **OtimizaÃ§Ã£o**: Insights para melhorar prompts/config
7. âœ… **DocumentaÃ§Ã£o**: Guia de quando usar cada tÃ©cnica

---

## ğŸ“ Notas Importantes

- âš ï¸ **NÃ£o mude as perguntas** entre tÃ©cnicas - use exatamente o mesmo texto
- âš ï¸ **Aguarde resposta completa** antes do prÃ³ximo teste
- âš ï¸ **Anote imediatamente** - nÃ£o confie na memÃ³ria depois
- âš ï¸ **Se der erro**, anote o erro e considere se deve repetir o teste
- âš ï¸ **Tempo total estimado**: ~2-3 horas (80 testes Ã— 2min mÃ©dia)

---

**Data do Teste**: ___________
**Testador**: ___________
**VersÃ£o**: v2.0 (Head-to-Head)
**Abordagem Escolhida**: [ ] Por Pergunta  [ ] Por TÃ©cnica
