# üß™ Sistema de Testes RAG Lab

Sistema completo para testar e comparar as 8 t√©cnicas RAG de forma objetiva.

## üìÅ Arquivos

- **TESTE_PLAN.md** - Plano detalhado com 10 perguntas universais e instru√ß√µes
- **TESTE_PLANILHA.csv** - Template de planilha para coleta manual
- **run_tests.py** - Script para execu√ß√£o automatizada via API
- **export_results.py** - Script para exportar resultados do banco para CSV

---

## üöÄ Op√ß√£o 1: Testes Manuais (Frontend)

### Vantagens
- ‚úÖ Visualiza respostas em tempo real
- ‚úÖ Melhor para an√°lise qualitativa
- ‚úÖ Interface amig√°vel

### Processo

1. **Abra o frontend**: http://localhost:9091

2. **Siga o TESTE_PLAN.md**:
   - 10 perguntas √ó 8 t√©cnicas = 80 testes
   - Escolha abordagem: Por Pergunta (recomendado) ou Por T√©cnica

3. **Para cada teste**:
   - Selecione t√©cnica no dropdown
   - Copie pergunta do TESTE_PLAN.md
   - Cole e envie
   - Aguarde resposta
   - Anote observa√ß√µes manualmente

4. **Ap√≥s completar**:
   ```bash
   python export_results.py
   ```
   - Gera `TESTE_RESULTS.csv` com m√©tricas autom√°ticas
   - Preencha colunas manuais: Qualidade (1-5), Relev√¢ncia (1-5), Observa√ß√µes

---

## ‚ö° Op√ß√£o 2: Testes Automatizados (Scripts)

### Vantagens
- ‚úÖ Muito mais r√°pido (1-2h vs 3h)
- ‚úÖ Consistente (sem erros de digita√ß√£o)
- ‚úÖ Rod√°vel em background

### Processo

**Rodar todos os testes (80)**:
```bash
python run_tests.py
```

**Testar apenas uma t√©cnica**:
```bash
python run_tests.py --technique baseline
python run_tests.py -t hyde
```

**Testar apenas uma pergunta**:
```bash
python run_tests.py --question 1  # Testa Q1 em todas t√©cnicas
python run_tests.py -q 5          # Testa Q5 em todas t√©cnicas
```

**Ajustar delay entre testes**:
```bash
python run_tests.py --delay 2.0   # 2 segundos entre testes
python run_tests.py -d 0.5        # 0.5 segundos (mais r√°pido)
```

**Exemplos combinados**:
```bash
# Testar apenas Agentic na Q1, sem delay
python run_tests.py -t agentic -q 1 -d 0

# Testar Baseline, HyDE e Reranking em todas perguntas, delay de 1.5s
python run_tests.py -t baseline && python run_tests.py -t hyde && python run_tests.py -t reranking -d 1.5
```

**Ap√≥s completar**:
```bash
python export_results.py
```

---

## üìä An√°lise de Resultados

### Exportar do Banco

```bash
python export_results.py
```

Gera `TESTE_RESULTS.csv` com:
- ‚úÖ M√©tricas autom√°ticas: lat√™ncia, num_sources, scores
- ‚è≥ Colunas manuais vazias: qualidade, relev√¢ncia, observa√ß√µes

### SQL Direto no Banco

**Estat√≠sticas por T√©cnica**:
```bash
cd backend
sqlite3 rag_lab.db << 'EOF'
SELECT
    technique,
    COUNT(*) as total,
    ROUND(AVG(JSON_EXTRACT(metrics, '$.latency_ms')), 2) as avg_latency,
    ROUND(AVG(JSON_EXTRACT(metrics, '$.num_sources')), 2) as avg_sources
FROM rag_executions
GROUP BY technique
ORDER BY avg_latency;
EOF
```

**Head-to-Head de 2 T√©cnicas**:
```bash
sqlite3 rag_lab.db << 'EOF'
SELECT
    r1.query as pergunta,
    ROUND(r1.latency, 2) as baseline_ms,
    ROUND(r2.latency, 2) as hyde_ms,
    ROUND(r2.latency - r1.latency, 2) as diff_ms
FROM
    (SELECT query, JSON_EXTRACT(metrics, '$.latency_ms') as latency
     FROM rag_executions WHERE technique = 'baseline') r1
JOIN
    (SELECT query, JSON_EXTRACT(metrics, '$.latency_ms') as latency
     FROM rag_executions WHERE technique = 'hyde') r2
ON r1.query = r2.query
ORDER BY diff_ms DESC;
EOF
```

---

## üéØ Workflow Recomendado

### Prepara√ß√£o
```bash
# 1. Limpar bancos (j√° feito!)
‚úÖ rag_lab.db: 0 registros
‚úÖ events.db: 0 eventos

# 2. Verificar servidores
curl http://localhost:8000/health  # Backend
curl http://localhost:9091         # Frontend

# 3. Verificar Pinecone
# Documentos j√° indexados? Se n√£o, indexe primeiro
```

### Execu√ß√£o

**Op√ß√£o A: Tudo automatizado**
```bash
python run_tests.py --delay 1.0
# ~2 horas, vai pra tomar caf√© ‚òï
```

**Op√ß√£o B: Por fases (recomendado)**
```bash
# Fase 1: T√©cnicas r√°pidas (baseline, hyde, reranking)
python run_tests.py -t baseline -d 0.5
python run_tests.py -t hyde -d 0.5
python run_tests.py -t reranking -d 1.0

# Fase 2: T√©cnicas m√©dias (fusion, graph)
python run_tests.py -t fusion -d 1.5
python run_tests.py -t graph -d 1.5

# Fase 3: T√©cnicas complexas (subquery, agentic, adaptive)
python run_tests.py -t subquery -d 2.0
python run_tests.py -t agentic -d 2.0
python run_tests.py -t adaptive -d 1.0
```

**Op√ß√£o C: Manual via frontend**
- Abra TESTE_PLAN.md
- Siga instru√ß√µes passo a passo
- Mais controle, mas mais demorado

### An√°lise

```bash
# 1. Exportar resultados
python export_results.py

# 2. Abrir TESTE_RESULTS.csv em Excel/Google Sheets

# 3. Preencher colunas manuais:
#    - Qualidade (1-5)
#    - Relev√¢ncia (1-5)
#    - Observa√ß√µes

# 4. Analisar:
#    - Ordenar por lat√™ncia
#    - Filtrar por t√©cnica
#    - Comparar qualidade vs lat√™ncia
#    - Identificar padr√µes
```

---

## üîç Validando Hip√≥teses

Ap√≥s coletar dados, valide as hip√≥teses do TESTE_PLAN.md:

**Performance**:
- [ ] H1: Baseline < HyDE < Reranking < Fusion < SubQuery?
- [ ] H2: Agentic tem lat√™ncia vari√°vel?
- [ ] H3: Adaptive adiciona ~100-200ms overhead?

**Qualidade**:
- [ ] H4: HyDE melhor em Q2, Q5 (conceituais)?
- [ ] H5: Reranking melhor em Q3, Q7 (precis√£o)?
- [ ] H6: Fusion melhor em Q5 (amb√≠guas)?
- [ ] H7: SubQuery melhor em Q8 (complexas)?
- [ ] H8: Graph melhor em Q6 (relacionamentos)?

**Comportamento**:
- [ ] H9: Agentic roteia corretamente?
- [ ] H10: Adaptive classifica bem?
- [ ] H11: Agentic itera quando necess√°rio?

**Scores**:
- [ ] H12: Reranking tem scores > Baseline?
- [ ] H13: HyDE pode ter scores menores mas respostas melhores?
- [ ] H14: Fusion normaliza scores via RRF?

---

## üìù Observa√ß√µes Importantes

### Durante os Testes
- ‚ö†Ô∏è Use perguntas **exatamente** como est√£o (copie/cole)
- ‚ö†Ô∏è Aguarde resposta completa antes do pr√≥ximo
- ‚ö†Ô∏è Se der erro, anote e considere repetir
- ‚ö†Ô∏è Lat√™ncia pode variar com carga do servidor/Pinecone

### An√°lise
- Compare **mesma pergunta** entre t√©cnicas (head-to-head)
- Lat√™ncia ‚â† Qualidade (nem sempre mais r√°pido √© melhor)
- Scores altos n√£o garantem boa resposta (veja relev√¢ncia)
- Agentic/Adaptive podem escolher t√©cnica diferente da selecionada

### Problemas Comuns

**Timeout/Erro 500**:
- Algumas t√©cnicas (SubQuery, Graph) s√£o mais lentas
- Aumente timeout no run_tests.py se necess√°rio
- Verifique logs do backend

**Scores muito baixos (<0.5)**:
- Pode ser falta de docs relevantes no Pinecone
- Verifique se documentos foram indexados
- Considere indexar mais conte√∫do

**Respostas gen√©ricas**:
- LLM pode estar gerando sem usar sources
- Verifique se sources est√£o vazios
- Pode ser problema no prompt do LLM

---

## üéØ Objetivos Finais

Ap√≥s os 80 testes, voc√™ ter√°:

1. ‚úÖ **Compara√ß√£o objetiva** de todas t√©cnicas
2. ‚úÖ **Trade-offs claros**: lat√™ncia vs qualidade vs custo
3. ‚úÖ **Guia de uso**: quando usar cada t√©cnica
4. ‚úÖ **Dados para otimiza√ß√£o**: ajustar prompts/config
5. ‚úÖ **Baseline confi√°vel**: para futuras melhorias

---

**Boa sorte nos testes! üöÄ**

Para d√∫vidas, consulte:
- TESTE_PLAN.md (detalhes completos)
- Backend logs (erros/debug)
- Frontend console (client-side issues)
